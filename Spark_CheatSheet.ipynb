{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnjMWYszBy2A"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.5.7/spark-3.5.7-bin-hadoop3.tgz\n",
        "\n",
        "!tar xf spark-3.5.7-bin-hadoop3.tgz\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.7-bin-hadoop3\"\n",
        "\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local[5]\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgVigkqlYSo-"
      },
      "source": [
        "---\n",
        "**SparkConf | SparkSession | SparkContext**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "collapsed": true,
        "id": "GBkbgPLOCfQQ",
        "outputId": "1d22b188-bfb9-49ad-9703-f5cb92e74b79"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://7aaf7bf9da5f:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.7</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[5]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Colab</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7cf154b8f740>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "##############################################################################\n",
        "################################# SPARK CONF #################################\n",
        "##############################################################################\n",
        "\n",
        "https://github.com/KaranBulani/spark_practise/blob/main/SBDL/conf/spark.conf\n",
        "\n",
        "[LOCAL]\n",
        "spark.app.name = sbdl-local\n",
        "spark.jars.packages = org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0\n",
        "[QA]\n",
        "spark.app.name = sbdl-qa\n",
        "spark.jars.packages = org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0\n",
        "spark.executor.cores = 5\n",
        "spark.executor.memory = 10GB\n",
        "spark.executor.memoryOverhead = 1GB\n",
        "spark.executor.instances = 20\n",
        "spark.sql.shuffle.partitions = 800\n",
        "[PROD]\n",
        "spark.app.name = sbdl\n",
        "spark.jars.packages = org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0\n",
        "spark.executor.cores = 5\n",
        "spark.executor.memory = 10GB\n",
        "spark.executor.memoryOverhead = 1GB\n",
        "spark.executor.instances = 20\n",
        "spark.sql.shuffle.partitions = 800\n",
        "'''\n",
        "Other imp configs:\n",
        "spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
        "\n",
        "spark.task.maxFailures      # Spark retries the failed task up to the number (default is 4) of times\n",
        "\n",
        "spark.sql.shuffle.partitions = 200 (default)\n",
        "# Dataframe API & Spark SQL - how many shuffle partitions Spark creates when doing wide transformations\n",
        "\n",
        "spark.shuffle.partitions (RDD API)\n",
        "# sc.getConf().set(\"spark.shuffle.partitions\", \"50\")\n",
        "# Controls number of partitions after reduceByKey, join, and other RDD shuffles.\n",
        "spark.shuffle.io.maxRetries   # number of times Spark will retry I/O operations during shuffle.\n",
        "\n",
        "spark.default.parallelism (RDD API) - If you have 4 executors, each with 5 cores, your cluster has 20 cores ‚Üí spark.default.parallelism = 20.\n",
        "\n",
        "# Case 1: RDD ReduceByKey (No partitions specified)\n",
        "# rdd = sc.textFile(\"data.txt\")\n",
        "# wordcounts = rdd.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
        "# This uses: spark.default.parallelism, Any Non shuffle operation will have this number of partitions.\n",
        "\n",
        "# Case 2: DataFrame GroupBy\n",
        "# df.groupBy(\"id\").count()\n",
        "# This uses: spark.sql.shuffle.partitions as it causes shuffle\n",
        "\n",
        "# driver config, ususally added spark-submit\n",
        "spark.driver.cores = 2                      # cores to allocate for your driver\n",
        "spark.driver.memory = 4G                    # JVM memory to allocate for your driver\n",
        "spark.driver.memoryOverhead = 1G            # memory for non-JVM off heap operation (Used by PySpark Driver)\n",
        "\n",
        "# spark.driver.memoryOverhead calculation can be done using below formula\n",
        "spark.driver.memoryOverhead = max(spark.driver.memory * spark.driver.memoryOverheadFactor, 384 MB)\n",
        "                            = max(4G * 0.10, 384)\n",
        "                            = 409.6 MB\n",
        "But we would do collect etc so we manually increased spark.driver.memoryOverhead to 1G\n",
        "\n",
        "# executor config, usually added spark.conf\n",
        "spark.executor.cores = 5                    # cpu cores for each executor, can improve parallelism and throughput.\n",
        "spark.executor.memory = 10GB                # JVM memory to allocate for each executor, Low executor memory typically causes out-of-memory (OOM) errors\n",
        "spark.executor.memoryOverhead = 1GB         # non-JVM memory for each executor\n",
        "spark.executor.instances = 20               # how many executors\n",
        "spark.sql.shuffle.partitions = 800\n",
        "\n",
        "# ratio which decides how spark.executor.memory is divided\n",
        "spark.memory.fraction = 0.6                 # % of memory used by spark\n",
        "spark.memory.storageFraction = 0.5          # % of memory protected from cache eviction\n",
        "spark.python.worker.memory                  # max for Py4J bridge\n",
        "\n",
        "# ratio which decides how spark.executor.memoryOverhead is divided\n",
        "# spark.executor.memoryOverheadFactor % used to calculate memoryOverhead\n",
        "spark.executor.memoryOverhead = max(spark.executor.memory * spark.executor.memoryOverheadFactor, 384 MB)\n",
        "                              = max(10G * 0.10, 384)\n",
        "                              = 1G\n",
        "\n",
        "# spark.executor.pyspark.memory - maximum for Python worker (Default 0)\n",
        "# spark.memory.offHeap.enabled = true - Disabled by default\n",
        "# spark.memory.offHeap.size : Memory reserved for off-heap allocations (Default 0)\n",
        "\n",
        "# ANALYZE TABLE is a SQL command used to compute table statistics, which the Catalyst optimizer uses to choose the most efficient execution plan.\n",
        "spark.sql(\"ANALYZE TABLE sales COMPUTE STATISTICS FOR COLUMNS product_id, price\")\n",
        "spark.sql(\"ANALYZE TABLE sales COMPUTE STATISTICS\")\n",
        "\n",
        "Adaptive Query Execution (AQE)\n",
        "spark.sql.adaptive.enabled = true (default)\n",
        "  # Dynamically coalesces shuffle partitions: Reduces partitions based on data size (avoids scheduling empty \"created due to spark.sql.shuffle.partitions\" or under utilized tasks)\n",
        "  # Dynamically switches join strategies: Converts sort-merge to broadcast join if small (shuffle stays, sort gets eliminated)\n",
        "  # Dynamically optimizes skew joins: Handle data skew during a join. Splits skewed partitions # Salting also does this but AQE does post shuffle whereas we implement salting pre shuffle\n",
        "  # Benefits: Better performance without manual tuning, adapts to runtime statistics\n",
        "\n",
        "spark.sql.adaptive.coalescePartitions.initialPartitionNum       # Starts with a set value; if not explicitly configured, it defaults to spark.sql.shuffle.partitions\n",
        "spark.sql.adaptive.coalescePartitions.minPartitionNum           # lower bound after coalescing; defaults to spark.default.parallelism if not set.\n",
        "# There isn‚Äôt a maxPartitionNum - Because: AQE only coalesces (i.e., reduces) shuffle partitions. It never increases the number of shuffle partitions.\n",
        "spark.sql.adaptive.advisoryPartitionSizeInBytes                 # (default: 64MB) ideal size for partitions during coalescing and splitting.\n",
        "spark.sql.adaptive.coalescePartitions.enabled = false           # (default: true) AQE will combine small partitions; setting it to false disables this feature.\n",
        "\n",
        "spark.sql.autoBroadcastJoinThreshold = 10MB\n",
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") # disables\n",
        "spark.sql.adaptive.localShuffleReader.enabled=true  (default)\n",
        "# Reduces network traffic by reading shuffle data locally\n",
        "\n",
        "spark.sql.adaptive.skewJoin.enabled = true                                      # Enable skew join optimization\n",
        "spark.sql.adaptive.skewJoin.skewedPartitionFactor (default - 5)                 # if its size exceeds five times the median partition size.\n",
        "spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes (default - 256MB)   # partition must exceed this size threshold to be considered skewed\n",
        "\n",
        "spark.sql.adaptive.maxShuffledHashJoinLocalMapThreshold                         # AQE converts a sort-merge join to a shuffled hash join when all post-shuffle partitions are smaller than the threshold\n",
        "\n",
        "Dynamic Partition Pruning\n",
        "spark.sql.optimizer.dynamicPartitionPruning.enabled (default True)\n",
        "\n",
        "Speculative Execution (Mitigate slow-running tasks: Launch duplicate \"speculative\" tasks on different nodes; first successful task is retained, others killed.)\n",
        "spark.speculation (default: false)\n",
        "\n",
        "Parameter | Default | Description\n",
        "spark.speculation.interval | 100 ms | Frequency of checking  for slow tasks.\n",
        "spark.speculation.multiplier | 1.5 | Runtime multiplier over median to flag a task as ‚Äúslow‚Äù. For a task running 5 sec 7.5 seconds is considered slow.\n",
        "spark.speculation.quantile | 0.75 / 75% | Fraction of tasks that must finish before speculation begins for a stage.\n",
        "spark.speculation.task.minRuntime | 100 ms | Minimum amount of time a task should run before it‚Äôs eligible for speculation. (avoids very short tasks).\n",
        "spark.speculation.task.maxTaskRuntime | (none) | Hard upper bound on task duration to trigger speculation unconditionally.\n",
        "\n",
        "Spark Scheduling Overview\n",
        "\n",
        "Across application ‚Äì how the cluster manager shares resources among multiple Spark apps.\n",
        "  Static Allocation (first-come, first reserve) (default)\n",
        "  Dynamic Allocation (Request executors  when needed Automatically Release idle executors back to cluster, Yarn RM has nothing to do here)\n",
        "\n",
        "spark.dynamicAllocation.enabled = true                        # disabled by default\n",
        "spark.dynamicAllocation.shuffleTracking.enabled = true        # enable shuffle tracking to prevent removing executors that still hold shuffle data.\n",
        "spark.shuffle.service.enabled = true                          # Or set this to true. purpose of the external shuffle service is to allow executors to be removed without deleting shuffle files written by them.\n",
        "spark.dynamicAllocation.executorIdleTimeout = 60s             # If an executor is not running any task for 60s, Spark Application will release executor back to cluster manager\n",
        "spark.dynamicAllocation.schedulerBacklogTimeout = 1s          # requests new executors if any tasks are pending for more than 1 second\n",
        "\n",
        "Within application ‚Äì how a single Spark app requests/releases executors over its lifetime while running multiple action/jobs.\n",
        "Sequential Execution (FIFO): By default, jobs run one after another (Job 1 ‚Üí finish ‚Üí Job 2 ‚Üí ‚Ä¶), even if they‚Äôre independent of each other.\n",
        "FAIR Scheduler: Tasks assigned in round-robin across all active jobs.\n",
        "                spark.conf.set(\"spark.scheduler.mode\", \"FAIR\")\n",
        "\n",
        "SPARK HISTORY SERVER (web UI that lets you view past Spark applications, Unlike the Spark UI which disappears after the application ends)\n",
        "spark.history.fs.logDirectory = hdfs:///spark-logs                  # for the History Server's storage (usually this & below are same)\n",
        "\n",
        "After enabling the Spark History Server, these two configuration properties are essential to configure your application to store event logs\n",
        "spark.eventLog.enabled  = true                                                         # turns on event logging\n",
        "spark.eventLog.dir      = hdfs:///spark-logs                                           # where to store the logs.\n",
        "\n",
        "\n",
        "remote/ -- remote is used internally to establish connection with Spark Connect\n",
        "\n",
        "'''\n",
        "import configparser\n",
        "from pyspark import SparkConf\n",
        "def get_spark_conf(env: str) -> SparkConf:\n",
        "    \"\"\"\n",
        "    Loads Spark configuration from conf/spark.conf file based on environment section.\n",
        "    \"\"\"\n",
        "    spark_conf = SparkConf()\n",
        "    config = configparser.ConfigParser()\n",
        "    config.read(\"conf/spark.conf\")\n",
        "\n",
        "    for (key, val) in config.items(env):\n",
        "        spark_conf.set(key, val)\n",
        "    return spark_conf\n",
        "\n",
        "##############################################################################\n",
        "################################ SPARK SESSION ###############################\n",
        "##############################################################################\n",
        "\"\"\"\n",
        "SparkSession acts as the driver. Entry point every program has it.\n",
        "Unifier for: SparkContext for core, SQLContext for SQL, HiveContext for Hive, StreamingContext for streaming.\n",
        "\n",
        "üîç What SparkSession fully unifies\n",
        "\n",
        "‚úî Spark SQL\n",
        "* Running SQL queries\n",
        "* Creating temp views\n",
        "* Catalog access\n",
        "* Table metadata\n",
        "* Parsing SQL automatically into DataFrame operations\n",
        "Example:\n",
        "spark.sql(\"SELECT * FROM customers WHERE age > 25\")\n",
        "\n",
        "‚úî Hive Integration\n",
        "When enabled, SparkSession becomes Hive-aware:\n",
        "spark.enableHiveSupport()\n",
        "spark.sql(\"SHOW DATABASES\")\n",
        "\n",
        "‚úî Catalog Access\n",
        "access to internal metadata:\n",
        "spark.catalog.listTables()\n",
        "spark.catalog.listDatabases()\n",
        "spark.catalog.listFunctions()\n",
        "\n",
        "‚úî Data Sources (Unified DataFrameReader & Writer & Transformer)\n",
        "SparkSession reads everything through one API:\n",
        "spark.read.csv()\n",
        "spark.read.json()\n",
        "spark.read.parquet()\n",
        "spark.read.format(\"delta\").load()\n",
        "spark.read.jdbc()\n",
        "\n",
        "You can create, transform, filter, group, join DataFrames:\n",
        "df = spark.read.csv(\"path.csv\", header=True, inferSchema=True)\n",
        "df.groupBy(\"country\").count().show()\n",
        "\n",
        "Same for writing.\n",
        "\n",
        "‚úî Underlying SparkContext\n",
        "SparkSession contains the original heart of Spark:\n",
        "sc = spark.sparkContext\n",
        "So it still supports RDD API:\n",
        "rdd = sc.parallelize([1,2,3])\n",
        "\n",
        "\"\"\"\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "def get_spark_session(env):\n",
        "    \"\"\"Create SparkSession based on environment\"\"\"\n",
        "    if env == \"LOCAL\":\n",
        "        return SparkSession.builder \\\n",
        "            .appName(\"PySpark App\") \\\n",
        "            .config(conf=get_spark_conf(env)) \\ # get_spark_conf defined above\n",
        "            .config('spark.sql.autoBroadcastJoinThreshold',-1) \\\n",
        "            .config('spark.sql.adaptive.enabled','false') \\\n",
        "            .config('spark.driver.extraJavaOptions','-Dlog4j.configuration=file:log4j.properties') \\ # pass JVM options to the Spark driver\n",
        "            .master(\"local[2]\") \\\n",
        "            .enableHiveSupport() \\\n",
        "            .getOrCreate()\n",
        "    else:\n",
        "        return SparkSession.builder \\\n",
        "            .config(conf=get_spark_conf(env)) \\ # get_spark_conf defined above\n",
        "            .enableHiveSupport() \\\n",
        "            .getOrCreate()\n",
        "'''\n",
        "The configuration of the SparkSession can be changed afterwards\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", 3)\n",
        "\n",
        "spark.conf.get(\"key\") method retrieves the value of a configuration property in a running PySpark session.\n",
        "spark.conf.getAll() method retrieves all configuration properties in a running PySpark session.\n",
        "'''\n",
        "##############################################################################\n",
        "\n",
        "spark = Utils.get_spark_session(job_run_env)\n",
        "\n",
        "'''\n",
        "Normal transformation\n",
        "'''\n",
        "spark.stop()\n",
        "'''\n",
        "This does:\n",
        "  Releases cluster resources\n",
        "  Closes executors\n",
        "  Cleans shuffle files\n",
        "  Stops SparkContext\n",
        "  Ends all SQL/Hive sessions\n",
        "'''\n",
        "\n",
        "##############################################################################\n",
        "################################ SPARK CONTEXT ###############################\n",
        "##############################################################################\n",
        "\n",
        "\"\"\"\n",
        "THEORY:\n",
        "DataFrame API is based on SparkSession, while the RDD API is based on SparkContext.\n",
        "\n",
        "\tfrom pyspark import SparkConf, SparkContext\n",
        "\t#Some logic where conf = SparkConf ()....\n",
        "\tsc = SparkContext (conf=conf)\n",
        "\n",
        "To use the RDD API, you need a SparkContext, which can be created with a SparkConf passed as parameter. However in newer Spark versions,\n",
        "  ‚óè instead of creating it separately , you can access the SparkContext from the SparkSession.\n",
        "  ‚óè SparkSession in newer version , is a higher-level abstraction that internally uses SparkContext .\n",
        "  ‚óè You can directly access SparkContext via SparkSession. Access SparkContext from SparkSession (SparkSession.sparkContext).\n",
        "\n",
        "#############################################################################\n",
        "\n",
        "USAGE 1:\n",
        "  Use SparkSession.sparkContext.getConf().getAll() to retrieve all configurations.\n",
        "  Used while creating logger https://github.com/KaranBulani/spark_practise/blob/main/09-RowDemo/lib/logger.py#L5\n",
        "\"\"\"\n",
        "from pyspark import SparkContext\n",
        "# OR\n",
        "from pyspark.sql import SparkSession.sparkContext\n",
        "\n",
        "#USAGE 2:\n",
        "sc = spark.sparkContext\n",
        "linesRDD = sc.textFile(sys.argv[1]) # returns RDD  where each record in RDD - a line from a text file\n",
        "\n",
        "# implement select(), filter(), groupBy() of dataframe with basic RDD function\n",
        "partitionedRDD = linesRDD.repartition(2)\n",
        "colsRDD = partitionedRDD.map(lambda line: line.replace('\"', '').split(\",\"))\n",
        "selectRDD = colsRDD.map(lambda cols: SurveyRecord(int(cols[1]), cols[2], cols[3], cols[4] ))\n",
        "filteredRDD = selectRDD.filter(lambda r: r.Age < 40)\n",
        "kvRDD = filteredRDD.map(lambda r: (r.Country, 1))\n",
        "countRDD = kvRDD.reduceByKey(lambda v1, v2: v1 + v2)\n",
        "\n",
        "colsList = countRDD.collect()\n",
        "\n",
        "\"\"\"\n",
        "USAGE 3 OUTDATED:\n",
        "    my_rows = [Row(\"123\",\"04/05/2020\"), Row(\"124\",\"4/5/2020\"), Row(\"125\", \"04/5/2020\"), Row(\"126\", \"4/05/2020\")]\n",
        "    my_rdd = spark.sparkContext.parallelize(my_rows, 2)\n",
        "    my_df = spark.createDataFrame(my_rdd, my_schema)\n",
        "\n",
        "Convert local list ‚Üí RDD\n",
        "Split the RDD into 2 partitions\n",
        "Let Spark process those partitions in parallel\n",
        "\n",
        "RATHER DO UPDATED:\n",
        "my_df = spark.createDataFrame(my_rows, my_schema).repartition(2)\n",
        "\n",
        "#############################################################################\n",
        "\n",
        "USAGE 4:\n",
        "  when using accumulator/ broadcast variables\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "degHmQjvGHeT"
      },
      "source": [
        "---\n",
        "**Schema | Reading Dataframe | Creating DataFrame using list, dict, pandas df**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCCzl_x5GSjy"
      },
      "outputs": [],
      "source": [
        "##############################################################################\n",
        "################################ SPARK SCHEMA ################################\n",
        "##############################################################################\n",
        "\n",
        "# Spark SQL Types\n",
        "# Data types for schema definition\n",
        "from pyspark.sql.types import (\n",
        "    StructType, StructField, StringType, IntegerType,\n",
        "    LongType, FloatType, DoubleType, BooleanType,\n",
        "    DateType, TimestampType, ArrayType, MapType, DecimalType\n",
        ")\n",
        "\n",
        "# Method 1: Using StructType\n",
        "# StructType(fields=None) - Struct type, consisting of a list of StructField\n",
        "# StructField(name, dataType, nullable=True, metadata=None)\n",
        "flight_schema_struct = StructType([\n",
        "    StructField(\"FL_DATE\", DateType(), True),\n",
        "    StructField(\"OP_CARRIER\", StringType(), True),\n",
        "    StructField(\"OP_CARRIER_FL_NUM\", IntegerType(), True),\n",
        "    StructField(\"ORIGIN\", StringType(), True),\n",
        "    StructField(\"ORIGIN_CITY_NAME\", StringType(), True),\n",
        "    StructField(\"DEST\", StringType(), True),\n",
        "    StructField(\"DEST_CITY_NAME\", StringType(), True),\n",
        "    StructField(\"CRS_DEP_TIME\", IntegerType(), True),\n",
        "    StructField(\"DEP_TIME\", IntegerType(), True),\n",
        "    StructField(\"WHEELS_ON\", IntegerType(), True),\n",
        "    StructField(\"TAXI_IN\", IntegerType(), True),\n",
        "    StructField(\"CRS_ARR_TIME\", IntegerType(), True),\n",
        "    StructField(\"ARR_TIME\", IntegerType(), True),\n",
        "    StructField(\"CANCELLED\", IntegerType(), True),\n",
        "    StructField(\"DISTANCE\", IntegerType(), True)\n",
        "])\n",
        "flight_schema_struct.add(\"new_column\",StringType(),True)\n",
        "\n",
        "# DataFrame.schema Returns the schema of this DataFrame as a pyspark.sql.types.StructType\n",
        "df.schema[\"column_name\"].dataType\n",
        "\n",
        "# Method 2: Using DDL String\n",
        " = \"\"\"\n",
        "    FL_DATE DATE, OP_CARRIER STRING, OP_CARRIER_FL_NUM INT, ORIGIN STRING,\n",
        "    ORIGIN_CITY_NAME STRING, DEST STRING, DEST_CITY_NAME STRING,\n",
        "    CRS_DEP_TIME INT, DEP_TIME INT, WHEELS_ON INT, TAXI_IN INT,\n",
        "    CRS_ARR_TIME INT, ARR_TIME INT, CANCELLED INT, DISTANCE INT\n",
        "\"\"\"\n",
        "\n",
        "##############################################################################\n",
        "################################# SPARK.READ #################################\n",
        "##############################################################################\n",
        "\n",
        "# DataFrameReader is returned from spark.read\n",
        "# https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.html#pyspark.sql.DataFrameReader\n",
        "\n",
        "# If it sounds like a Spark DataFrame feature ‚Üí camelCase.\n",
        "# If it sounds like a basic file-read/write behavior ‚Üí lowercase.\n",
        "\n",
        "# wholetext option to read each file as one row and captures its path with input_file_name()\n",
        "# from pyspark.sql.functions import input_file_name\n",
        "# df = (spark.read\n",
        "#           .format(\"text\")\n",
        "#           .option(\"wholetext\", \"true\")\n",
        "#           .load(\"s3a://bucket/dir/*\")\n",
        "#           .select(\n",
        "#               input_file_name().alias(\"path\"),\n",
        "#               col(\"value\").alias(\"content\")\n",
        "#           ))\n",
        "\n",
        "def read_data_examples(spark: SparkSession):\n",
        "# Small stage created for inferSchema\n",
        "    # CSV with schema\n",
        "    csv_df = spark.read \\\n",
        "        .format(\"csv\") \\\n",
        "        .option(\"header\", \"true\") \\\n",
        "        .option(\"delimiter\", \"\\t\") \\ # Read with custom delimiter\n",
        "        .schema(flight_schema_struct) \\ # Can use .option(\"inferSchema\", \"true\")  if confident about schema\n",
        "        .option(\"mode\", \"FAILFAST\") \\ # mode is option in spark.read whereas not an option in spark.write\n",
        "        .option(\"dateFormat\", \"M/d/y\") \\\n",
        "        .load(\"data/flight*.csv\")\n",
        "\n",
        "# mode options:\n",
        "# PERMISSIVE (default: set corrupted field to null, Stores corrupted records  in a _corrupt_record column of Dataframe)\n",
        "# DROPMALFORMED (loads only valid ones),\n",
        "# FAILFAST (Raises an exception and halts execution)\n",
        "\n",
        "    # JSON with DDL schema\n",
        "    json_df = spark.read \\\n",
        "        .format(\"json\") \\\n",
        "        .schema(flight_schema_ddl) \\\n",
        "        .option(\"dateFormat\", \"M/d/y\") \\\n",
        "        .load(\"data/flight*.json\")\n",
        "\n",
        "    # Parquet (schema inferred)\n",
        "    parquet_df = spark.read \\\n",
        "        .format(\"parquet\") \\\n",
        "        .load(\"data/flight*.parquet\")\n",
        "\n",
        "    bq_df = spark.read \\\n",
        "        .format(\"bigquery\") \\\n",
        "        .option(\"filter\", \"_PARTITIONDATE > '2019-01-01'\") \\\n",
        "        .load(\"bigquery-public-data.samples.shakespeare\")\n",
        "\n",
        "    return csv_df, json_df, parquet_df, bq_df\n",
        "\n",
        "# Alternative shorthand syntax\n",
        "csv_df2 = spark.read.csv(\"data/*.csv\", header=True, inferSchema=True, sep=\"|\")\n",
        "json_df2 = spark.read.json(\"data/*.json\")\n",
        "parquet_df2 = spark.read.option(\"mergeSchema\", \"true\").parquet(\"/mnt/data/parquet_dir\") # A directory with multiple Parquet files, where each file has a slightly different but compatible schema. mergeSchema option merges all into a single DataFrame\n",
        "# mergeSchema option prevents failures on mismatched schemas.\n",
        "\n",
        "# to read Parquet files with nested folders:\n",
        "spark.read.parquet(\"root/2024/*/*\")\n",
        "# Wildcard (*) usage:\n",
        "# Single * ‚Üí matches one directory level.\n",
        "# Double * ‚Üí matches two levels (e.g., month and day).\n",
        "# Spark supports wildcards in paths when reading data.\n",
        "\n",
        "# Read from Hive table\n",
        "hive_df = spark.read.table(\"database.table_name\")\n",
        "\n",
        "# Read with custom delimiter\n",
        "tsv_df = spark.read \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"delimiter\", \"\\t\") \\\n",
        "    .csv(\"data/file.tsv\")\n",
        "# csv() Comma Seperated Value can also read tsv() Tab Seperated Value.\n",
        "\n",
        "##############################################################################\n",
        "############################ SPARK.createDataFrame ###########################\n",
        "##############################################################################\n",
        "\n",
        "def create_dataframe_examples(spark: SparkSession):\n",
        "    \"\"\"\n",
        "    Examples of creating DataFrames from Python data structures\n",
        "    \"\"\"\n",
        "    from datetime import date\n",
        "\n",
        "    # Method 1: From list of tuples with explicit schema\n",
        "    data = [\n",
        "        (date(2024, 1, 1), \"AA\", 100, \"JFK\", \"New York, NY\", \"LAX\", \"Los Angeles, CA\", 800, 805, 1130, 15, 1145, 1145, 0, 2475),\n",
        "        (date(2024, 1, 2), \"DL\", 200, \"ATL\", \"Atlanta, GA\", \"ORD\", \"Chicago, IL\", 900, 910, 1045, 10, 1100, 1055, 0, 606),\n",
        "        (date(2024, 1, 3), \"UA\", 300, \"SFO\", \"San Francisco, CA\", \"SEA\", \"Seattle, WA\", 1200, None, None, None, 1400, None, 1, 679)\n",
        "    ]\n",
        "\n",
        "    df_with_schema = spark.createDataFrame(data, schema=flight_schema_struct)\n",
        "\n",
        "    # Method 2: From list of dictionaries (schema inferred)\n",
        "    data_dict = [\n",
        "        {\"FL_DATE\": date(2024, 1, 1), \"OP_CARRIER\": \"AA\", \"OP_CARRIER_FL_NUM\": 100,\n",
        "         \"ORIGIN\": \"JFK\", \"DEST\": \"LAX\", \"DISTANCE\": 2475},\n",
        "        {\"FL_DATE\": date(2024, 1, 2), \"OP_CARRIER\": \"DL\", \"OP_CARRIER_FL_NUM\": 200,\n",
        "         \"ORIGIN\": \"ATL\", \"DEST\": \"ORD\", \"DISTANCE\": 606}\n",
        "    ]\n",
        "\n",
        "    df_from_dict = spark.createDataFrame(data_dict)\n",
        "\n",
        "    # Method 3: From pandas DataFrame\n",
        "    # A major difference is evaluation model:\n",
        "    # Spark DataFrames ‚Üí lazy evaluation (plan is built, executed only on action).\n",
        "    # Pandas DataFrames ‚Üí eager evaluation, producing immediate results, which is helpful for prototyping and debugging on smaller data.\n",
        "\n",
        "    import pandas as pd\n",
        "\n",
        "    pandas_df = pd.DataFrame({\n",
        "        \"FL_DATE\": [date(2024, 1, 1), date(2024, 1, 2)],\n",
        "        \"OP_CARRIER\": [\"AA\", \"DL\"],\n",
        "        \"OP_CARRIER_FL_NUM\": [100, 200],\n",
        "        \"ORIGIN\": [\"JFK\", \"ATL\"],\n",
        "        \"DEST\": [\"LAX\", \"ORD\"],\n",
        "        \"DISTANCE\": [2475, 606]\n",
        "    })\n",
        "\n",
        "    df_from_pandas = spark.createDataFrame(pandas_df)\n",
        "\n",
        "    # Method 4: From RDD with schema\n",
        "    rdd = spark.sparkContext.parallelize(data)\n",
        "    df_from_rdd = spark.createDataFrame(rdd, schema=flight_schema_struct)\n",
        "\n",
        "    # Method 5: Empty DataFrame with schema\n",
        "    empty_df = spark.createDataFrame([], schema=flight_schema_struct)\n",
        "\n",
        "    return df_with_schema, df_from_dict, df_from_pandas, df_from_rdd, empty_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3EFJCvl8_jR"
      },
      "source": [
        "---\n",
        "**Dataframe Write | Coalesce, Repartition (Transformation)**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64nlCqcoEvtg"
      },
      "outputs": [],
      "source": [
        "##############################################################################\n",
        "################################ SPARK.WRITE #################################\n",
        "##############################################################################\n",
        "\n",
        "# DataFrameWriter is returned from spark.write\n",
        "# https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.html\n",
        "\n",
        "'''\n",
        "If the setting is generic across all sources/sinks ‚Üí it has a direct function.\n",
        "If Spark needs the setting to decide HOW to perform read/write ‚Üí it's a direct function (e.g., format, schema, mode)\n",
        "\n",
        "If the setting is source-specific or sink-specific ‚Üí it goes inside .option()\n",
        "If Spark needs the setting to configure a particular format ‚Üí it's an option (e.g., header, delimiter, multiline, bootstrap.servers)\n",
        "\n",
        "| Method                                          | When used          | Why it's not an option                         |\n",
        "| ----------------------------------------------- | ------------------ | ---------------------------------------------- |\n",
        "|  .format(\"csv\")                                 | Select data source | Core API ‚Äî choosing the backend implementation |\n",
        "|  .mode(\"overwrite\")                             | Save mode          | Applies to *all* writes                        |\n",
        "|  .schema(mySchema)                              | Define schema      | Works for any structured source                |\n",
        "|  .load()  /  .save()                            | Trigger IO         | Core action/method                             |\n",
        "|  .option(\"path\", ...)  (write only alternative) | Path shortcut      | Special exception                              |\n",
        "'''\n",
        "\n",
        "# Partitioned write with max records per file\n",
        "df.write \\\n",
        "    .format(\"json\") \\ # Internal (CSV, JSON, Parquet(default), AVRO, ORC) vs External (JDBC, Cassandra, MongoDB, Kafka, Delta Lake)\n",
        "    .mode(\"overwrite\") \\ # Write modes NOT SET VIA OPTION\n",
        "    .option(\"path\", \"output/json/\") \\ # Minimum requirement : Specify the target\n",
        "    .partitionBy(\"OP_CARRIER\", \"ORIGIN\") \\ # Each partition of the DataFrame creates one output file, regardless of executors. Unless mentioned partitionBy\n",
        "    .option(\"maxRecordsPerFile\", 10000) \\ # Limits the number of records per file\n",
        "    .save() #Path can be mentioned here # action\n",
        "\n",
        "# Write modes:\n",
        "# - error or errorIfExists: Throws error if data exists (default)\n",
        "# - append: Adds new data without modifying existing files\n",
        "# - overwrite: Deletes existing files and writes new ones\n",
        "# - ignore: Writes only if target is empty\n",
        "\n",
        "# DataFrameWriter - does not support any ordering options.\n",
        "# Spark writes data in parallel, and ordering requires a global shuffle, which is not guaranteed when writing multiple files.\n",
        "# If you want the entire dataset globally sorted, you can do: df.orderBy(\"partion_columns_used_during_write\",\"some_other_column\") This some_other_column should not be used in partition by.\n",
        "# But Spark cannot guarantee file-level sort\n",
        "# Even if you sort before writing, Spark still writes data in parallel, so:\n",
        "# Files can contain sorted chunks\n",
        "# But file boundaries are not guaranteed\n",
        "# Within each file, ordering may break depending on shuffle partitions\n",
        "# If you must have fully sorted single file output then disable parallelism:\n",
        "df.orderBy(\"some_column\") \\\n",
        "  .coalesce(1) \\\n",
        "  .write \\\n",
        "  .format(\"json\") \\\n",
        "  .mode(\"overwrite\") \\\n",
        "  .save(\"output/json/\")\n",
        "\n",
        "# For DataFrameWriter.text - DataFrame must have only one column that is of string type. Each row becomes a new line in the output file.\n",
        "\n",
        "\n",
        "# Coalesce before write (reduce number of output files by merging partitions on the same worker) returns original DataFrame if target > current\n",
        "# If the single coalesced partition does not fit in executor memory: Spark will spill data to disk\n",
        "# If it still cannot handle it: Task fails with OOM / GC error\n",
        "# After retries: The entire job fails\n",
        "df.coalesce(1).write.csv(\"output/single_file.csv\")\n",
        "\n",
        "# Repartition before write (increase parallelism) also controls the number of output files. Blind repartitioning (not on basis of a column). Create empty partition if repartition count > number of rows\n",
        "# repartition(numPartitions, *cols) - Returns a new DataFrame partitioned by the given partitioning expressions. The resulting DataFrame is hash partitioned.\n",
        "# numPartitions - can be an int to specify the target number of partitions or a Column. If it is a Column, it will be used as the first partitioning column. If not specified, the default number of partitions is used.\n",
        "# cols - str or Column - partitioning columns.\n",
        "df.repartition(10).write.parquet(\"output/repartitioned\")\n",
        "df.repartition(\"category\").write.parquet(\"output/repartitioned\") # Spark decides the number of partitions based on the shuffle.\n",
        "df.repartition(8, \"category\").write.parquet(\"output/repartitioned\") # there may be more categories than partitions but 8 partitions only\n",
        "\n",
        "# Check partitions\n",
        "df.rdd.getNumPartitions()\n",
        "\n",
        "# Write with compression\n",
        "df.write \\\n",
        "    .option(\"compression\", \"snappy\") \\\n",
        "    .parquet(\"output/compressed\") # action\n",
        "# Acceptable compression values include: none, uncompressed, snappy, gzip, lzo, brotli, lz4, lz4_raw, zstd. Note that brotli requires BrotliCodec to be installed.\n",
        "\n",
        "# In this method the data is written first to GCS, and then it is loaded it to BigQuery. A GCS bucket must be configured to indicate the temporary data location.\n",
        "df.write \\\n",
        "  .format(\"bigquery\") \\\n",
        "  .option(\"temporaryGcsBucket\",\"some-bucket\") \\\n",
        "  .save(\"dataset.table\") # action\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MciAT6dnFEfq"
      },
      "source": [
        "---\n",
        "**Dataframe Write DB | BucketBy, sortBy**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "collapsed": true,
        "id": "LXpr0GZIEyGv",
        "outputId": "e271ce94-35bd-40fc-abee-34c7ab0a9f09"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'spark' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1596381636.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CREATE DATABASE IF NOT EXISTS AIRLINE_DB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetCurrentDatabase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AIRLINE_DB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# bucketBy & sortBy only works with Hive and needs saveAsTable()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# .enableHiveSupport() in SparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
          ]
        }
      ],
      "source": [
        "spark.sql(\"CREATE DATABASE IF NOT EXISTS AIRLINE_DB\")\n",
        "spark.catalog.setCurrentDatabase(\"AIRLINE_DB\")\n",
        "# spark.sql(\"USE AIRLINE_DB\")\n",
        "\n",
        "# bucketBy & sortBy only works with Hive and needs saveAsTable(), bucket + sort improves join operation (eliminating shuffle & Bucketed datasets can be joined multiple times without additional shuffling)\n",
        "# also keep .enableHiveSupport() in SparkSession\n",
        "\n",
        "# Bucketing: Best for optimizing joins and ensuring shuffle reduction (on columns like customer_id). As records with the same key are placed in the same bucket\n",
        "# Partitioning: Best for filter-based pruning (e.g., year, month, region).\n",
        "# Too fine-grained partitioning (like date): Creates small files and metadata overhead.\n",
        "\n",
        "# Each partition folder contains bucketed files.\n",
        "# Creates a directory for each partition value (date):\n",
        "# Within each partition folder, rows are hashed into N buckets. If no partition is mentioned then just N buckets. Align N buckets with N node cluster\n",
        "\n",
        "\n",
        "# Rest all DataFrame write is same\n",
        "\n",
        "df.write \\\n",
        "  .mode(\"overwrite\") \\\n",
        "  .partitionBy(\"date\") \\\n",
        "  .bucketBy(32, \"customer_id\") \\\n",
        "  .sortBy(\"customer_id\") \\\n",
        "  .saveAsTable(\"sales_fact\") #table_name\n",
        "\n",
        "# This is created as a managed Table. Once we mention path it becomes external table.\n",
        "# Managed table ‚Üí Spark controls both data + metadata ‚Üí No path specified\n",
        "# External table ‚Üí Spark controls only metadata ‚Üí Path is specified\n",
        "# If you are dropping an unmanaged table, no data will be removed but you will no longer be able to refer to this data by the table name.\n",
        "\n",
        "\n",
        "print(spark.catalog.listTables(\"AIRLINE_DB\"))\n",
        "# https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Catalog.html\n",
        "# catalog is abstraction for the storage of metadata about the data stored in your tables as well as other helpful things like databases, tables, functions, and views\n",
        "spark.catalog.cacheTable(\"tbl1\")\n",
        "spark.catalog.uncacheTable(‚Äútable‚Äù)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4itTRPgH2reX"
      },
      "source": [
        "---\n",
        "**Spark SQL**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDWq2J0y2oRq"
      },
      "outputs": [],
      "source": [
        "# Create temporary view (accessible within current SparkSession)\n",
        "df.createOrReplaceTempView(\"sales_view\")\n",
        "result = spark.sql(\"\"\"\n",
        "    SELECT Country, SUM(Quantity) as total_qty\n",
        "    FROM sales_view\n",
        "    WHERE InvoiceDate >= '2010-01-01'\n",
        "    GROUP BY Country\n",
        "    ORDER BY total_qty DESC\n",
        "\"\"\")\n",
        "# Spark SQL does not guarantee the order in which subexpressions are evaluated.\n",
        "\n",
        "spark.read.table(\"sales_view\") # Not correct as per AI\n",
        "spark.table(\"sales_view\")\n",
        "\n",
        "# Create global temp view (accessible across SparkSessions)\n",
        "df.createGlobalTempView(\"global_sales\")\n",
        "spark.sql(\"SELECT * FROM global_temp.global_sales\").show()\n",
        "spark.read.table(\"global_temp.global_sales\")\n",
        "spark.table(\"global_temp.global_sales\")\n",
        "\n",
        "# You have a Parquet dataset at path /mnt/data/some_file.parquet. Without creating a table, how do you query it directly\n",
        "# SELECT * FROM parquet.`/mnt/data/event_file.parquet`;\n",
        "\n",
        "# USING IS KEPT IN CREATE TABLE STATEMENT\n",
        "# CREATE TABLE my_table (   id INT,  name STRING,  salary DOUBLE ) USING PARQUET;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3WLv9i7UXRc"
      },
      "source": [
        "---\n",
        "**Join** + **Broadcast**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "collapsed": true,
        "id": "Vx289khUFDLp",
        "outputId": "bd253543-a0b1-4ffa-a095-a01bf411d875"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n| Spark Join Type  | All Aliases (identical)            | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t                           \\tMeaning \\t\\t\\t\\t\\t\\t        \\t\\t\\t\\t\\t\\t\\t                        |\\n| ---------------- | ---------------------------------- | ------------------------------------------------------------------------------------------------------------------------- |\\n|   inner(default) | inner                              | Returns only rows that have matching keys in both DataFrames. \\t\\t\\t\\t\\t\\t\\t                            \\t\\t\\t\\t\\t\\t\\t  \\t|\\n|   cross          | cross                              | Cartesian product ‚Üí every row in A combined with every row in B. \\t\\t\\t\\t\\t                            \\t\\t\\t\\t\\t\\t\\t\\t\\t  |\\n|   full outer     | full, outer, fullouter, full_outer | Returns all rows from both DataFrames. Where keys don't match, unmatched columns become NULL.                             |\\n|   left outer     | left, leftouter, left_outer        | Returns all rows from left DataFrame and matching rows from right. Unmatched right rows ‚Üí NULL.\\t\\t\\t\\t\\t\\t\\t              |\\n|   right outer    | right, rightouter, right_outer     | Opposite of left join. Returns all rows from right, and matching from left.\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t                        |\\n|   left semi      | semi, leftsemi, left_semi          | Returns only left-side rows that have at least one match on the right. Does not include any columns from right DataFrame. |\\n|   left anti      | anti, leftanti, left_anti          | Returns only left-side rows where there is NO match on right.\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t                                |\\n\""
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.join.html#pyspark.sql.DataFrame.join\n",
        "DataFrame.join(other, on=None, how=None)\n",
        "| Spark Join Type  | All Aliases (identical)            | \t\t\t\t\t\t\t\t\t\t\t\t\t\t                           \tMeaning \t\t\t\t\t\t        \t\t\t\t\t\t\t                        |\n",
        "| ---------------- | ---------------------------------- | ------------------------------------------------------------------------------------------------------------------------- |\n",
        "|   inner(default) | inner                              | Returns only rows that have matching keys in both DataFrames. \t\t\t\t\t\t\t                            \t\t\t\t\t\t\t  \t|\n",
        "|   cross          | cross                              | Cartesian product ‚Üí every row in A combined with every row in B. \t\t\t\t\t                            \t\t\t\t\t\t\t\t\t  |\n",
        "|   full outer     | full, outer, fullouter, full_outer | Returns all rows from both DataFrames. Where keys don't match, unmatched columns become NULL.                             |\n",
        "|   left outer     | left, leftouter, left_outer        | Returns all rows from left DataFrame and matching rows from right. Unmatched right rows ‚Üí NULL.\t\t\t\t\t\t\t              |\n",
        "|   right outer    | right, rightouter, right_outer     | Opposite of left join. Returns all rows from right, and matching from left.\t\t\t\t\t\t\t\t\t\t\t\t                        |\n",
        "|   left semi      | semi, leftsemi, left_semi          | Returns only left-side rows that have at least one match on the right. Does not include any columns from right DataFrame. If there is no matching row on the right, Spark removes that left-row from the result. |\n",
        "|   left anti      | anti, leftanti, left_anti          | Returns only left-side rows where there is NO match on right. keep NON-matches\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t              |\n",
        "'''\n",
        "##############################################################################\n",
        "#################################### JOIN ####################################\n",
        "##############################################################################\n",
        "join_expr = order_df.prod_id == product_df.prod_id\n",
        "joined_df = order_df.join(product_df, join_expr, \"inner\")\n",
        "\n",
        "# Alternative join syntax\n",
        "joined_df = order_df.join(product_df, \"prod_id\", \"inner\") #\"prod_id\"should be present with same name on both df\n",
        "joined_df = order_df.join(product_df, [\"prod_id\", \"category\"], \"inner\")\n",
        "\n",
        "# Left Semi Join (returns only left side where match exists)\n",
        "semi_join_df = order_df.join(product_df, join_expr, \"left_semi\")\n",
        "\n",
        "# Left Anti Join (returns only left side where no match)\n",
        "anti_join_df = order_df.join(product_df, join_expr, \"left_anti\")\n",
        "\n",
        "# Cross Join (Cartesian product)\n",
        "df_cross = df1.join(df2, how=\"cross\")\n",
        "cross_join_df = order_df.crossJoin(product_df)\n",
        "\n",
        "from pyspark.sql.functions import expr\n",
        "\n",
        "# Complex Example\n",
        "def join_examples(order_df: DataFrame, product_df: DataFrame):\n",
        "    join_expr = order_df.prod_id == product_df.prod_id\n",
        "    product_renamed = product_df.withColumnRenamed(\"qty\", \"reorder_qty\")\n",
        "\n",
        "    # Inner Join\n",
        "    order_df.join(product_renamed, join_expr, \"inner\") \\\n",
        "        .drop(product_renamed.prod_id) \\\t# Handle duplicate column names after join\n",
        "        .select(\"order_id\", \"prod_id\", \"prod_name\", \"unit_price\", \"list_price\", \"qty\") \\\n",
        "        .show()\n",
        "\n",
        "    # Left Join with coalesce + sort\n",
        "    order_df.join(product_renamed, join_expr, \"left\") \\\n",
        "        .drop(product_renamed.prod_id) \\\n",
        "        .select(\"order_id\", \"prod_id\", \"prod_name\", \"unit_price\", \"list_price\", \"qty\") \\\n",
        "        .withColumn(\"prod_name\", expr(\"coalesce(prod_name, prod_id)\")) \\\t\t\t# Join with coalesce for null handling\n",
        "        .withColumn(\"list_price\", expr(\"coalesce(list_price, unit_price)\")) \\\t\t# Join with coalesce for null handling\n",
        "        .sort(\"order_id\") \\\n",
        "        .show()\n",
        "\n",
        "##############################################################################\n",
        "################################# BROADCAST ##################################\n",
        "##############################################################################\n",
        "\n",
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "# Broadcast Join (for small tables)\n",
        "join_expr = large_df.id == small_df.id\n",
        "broadcast_join_df = large_df.join(broadcast(small_df), join_expr, \"inner\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9czY1rv2D6c"
      },
      "source": [
        "---\n",
        "**Transformations (Narrow/Wide/Window)**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "collapsed": true,
        "id": "JktOapoUUWY5",
        "outputId": "d7a6022f-5b1a-49f7-bb57-3a055575828c"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-3366183906.py, line 123)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-3366183906.py\"\u001b[0;36m, line \u001b[0;32m123\u001b[0m\n\u001b[0;31m    Null elements will be placed at the beginning of the returned array in ascending order or at the end of the returned array in descending order.\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "##############################################################################\n",
        "############################ Basic Transformation ############################\n",
        "##############################################################################\n",
        "\n",
        "from pyspark.sql.functions import col, expr, lit, concat, asc, desc\n",
        "\n",
        "# Select columns\n",
        "df.select(\"col1\", \"col2\", \"col3\")\n",
        "df.select(col(\"col1\"), col(\"col2\"))\n",
        "df.select(col(\"col1\").alias(\"new_name\"))\n",
        "# Note these also work instead of col(\"a\")\n",
        "# df.col1\n",
        "# df[\"a\"]\n",
        "\n",
        "# df can be aliased too\n",
        "# customers = customers.alias('c')\n",
        "# then when using col() it would be col(\"c.id\")\n",
        "\n",
        "df.selectExpr(\n",
        "    \"name\",\n",
        "    \"age\",\n",
        "    \"CASE WHEN age >= 35 THEN 'Senior' ELSE 'Junior' END as age_category\"\n",
        ").show()\n",
        "\n",
        "\n",
        "# Select with expressions\n",
        "df.select(\n",
        "    col(\"name\"),\n",
        "    (col(\"price\") * 1.1).alias(\"price_with_tax\"),\n",
        "    expr(\"price * 1.1 as price_with_tax2\")\n",
        ")\n",
        "\n",
        "# Filter / Where\n",
        "df.filter(\"age > 21\")\n",
        "df.filter(col(\"age\") > 21)\n",
        "df.filter(col(\"age\").between(18, 30))\n",
        "df.where(col(\"status\") == \"active\")\n",
        "df.where((col(\"age\") > 21) & (col(\"country\") == \"USA\")) # & AND\n",
        "df.where((col(\"age\") > 21) | (col(\"country\") == \"USA\")) # | OR\n",
        "df.where(~col(\"first_name\").isin(\"Jill\", \"Eva\")) # ~ NOT\n",
        "df.where(col(\"country\").isin(\"USA\",\"INDIA\"))\n",
        "df.where(col(\"name\").like(\"J%\")) # Normal Like\n",
        "df.where(col(\"name\").rlike(\"^J.*\")) # Regex Like\n",
        "df.where(col(\"name\").contains(\"ill\")) # '%ill%'\n",
        "df.where(col(\"name\").startswith(\"Ji\"))\n",
        "df.where(col(\"name\").endswith(\"va\"))\n",
        "\n",
        "df.where(\"CallType is not null\")\n",
        "\n",
        "# Add new column # In withColumn(), the expression you provide must evaluate to exactly one value per row. \n",
        "df.withColumn(\"new_col\", lit(\"constant_value\"))\n",
        "df.withColumn(\"price_doubled\", col(\"price\") * 2)\n",
        "df.withColumn(\"full_name\", concat(col(\"first_name\"), lit(\" \"), col(\"last_name\")))\n",
        "df.withColumns({\"age1\": lit(10), \"age2\": lit(20)}) # withColumns(*colsMap)\n",
        "\n",
        "# Rename column NOTE ALL ARE CAMELCASE df.\n",
        "df.withColumnRenamed(\"old_name\", \"new_name\") # withColumnRenamed(existing, new)\n",
        "df.withColumnsRenamed({\"id\": \"identifier\", \"name\": \"full_name\"}) # withColumnsRenamed(colsMap)\n",
        "\n",
        "# Drop columns drop(*cols)\n",
        "df.drop(\"col1\", \"col2\")\n",
        "df.drop(col(\"col1\"))\n",
        "\n",
        "# Drop rows dropna(how='any', thresh=None, subset=None)\n",
        "# how: any or all\n",
        "# thresh: int - drop rows that have less than thresh non-null values. This overwrites the how parameter.\n",
        "# subset: optional list of column names to consider.\n",
        "df.dropna()               # how='any'\n",
        "df.dropna(how='all')\n",
        "df.dropna(thresh=2)       # less than 2 non-null values stays, rest (2,3,4) gets dropped\n",
        "df.dropna(subset=[\"name\", \"age\"]).show()          # returns rows where both are not null\n",
        "\n",
        "# df.na returns DataFrameNaFunctions\n",
        "# DataFrameNaFunctions.drop(how='any', thresh=None, subset=None) Returns a new DataFrame omitting rows with null or NaN values.\n",
        "# DataFrame.dropna() and DataFrameNaFunctions.drop()\n",
        "df.na.drop(how=\"any\")\n",
        "df.na.fill(50) # Fill all null values with 50 for numeric columns. (same for all other datatype)\n",
        "df.na.fill({'age': 50, 'name': 'unknown'})\n",
        "df.na.fill(50, subset=[\"age\"])\n",
        "\n",
        "# fillna(value, subset=None)\n",
        "# value: value to replace null values with int/str/float.\n",
        "# subset: list of column names to consider.\n",
        "df.fillna(\"N/A\", subset=[\"name\", \"city\"])\n",
        "df.fillna(\"default_value\") # will only fill where datatype is matching\n",
        "\n",
        "# Drop duplicates dropDuplicates(subset=None) subset list of column names\n",
        "df.dropDuplicates() #drop_duplicates both same\n",
        "df.dropDuplicates([\"col1\", \"col2\"])\n",
        "df.distinct()\n",
        "\n",
        "# Sort / OrderBy\n",
        "df.sort(\"col1\", \"col2\")\n",
        "df.sort(col(\"col1\").asc(), col(\"col2\").desc())\n",
        "df.orderBy(\"count\", ascending=False)\n",
        "\n",
        "df.sort(\"x\", desc(\"y\"))\n",
        "df.sort(asc(\"x\"), desc(\"y\"))\n",
        "df.orderBy(col(\"x\").asc(), col(\"y\").desc())\n",
        "df.orderBy([\"x\", \"y\"], ascending=[True, False])\n",
        "'''\n",
        "| Function                  | Sort Direction | Null Position | Usage Example                       |\n",
        "| ------------------------- | -------------- | ------------- | ----------------------------------- |\n",
        "|   asc_nulls_first(col)    | Ascending      | Nulls first   | df.orderBy(asc_nulls_first(\"age\"))  |\n",
        "|   asc_nulls_last(col)     | Ascending      | Nulls last    | df.orderBy(asc_nulls_last(\"age\")), df.orderBy(df.a.asc_nulls_last()), df.sort(df.a.asc_nulls_last()), df.sort(asc_nulls_last(\"a\"))  |\n",
        "|   desc_nulls_first(col)   | Descending     | Nulls first   | df.orderBy(desc_nulls_first(\"age\")) |\n",
        "|   desc_nulls_last(col)    | Descending     | Nulls last    | df.orderBy(desc_nulls_last(\"age\"))  |\n",
        "'''\n",
        "\n",
        "# Limit\n",
        "df.limit(10)\n",
        "\n",
        "# DataFrame.sample(withReplacement=None, fraction=None, seed=None)\n",
        "# Returns a sampled subset of this DataFrame. Creating smaller portions of a large dataset for testing, development, or analysis.\n",
        "# fraction : float, optional - Probability of including each row [0.0, 1.0]. It is a probability, not an exact count. So fraction=0.1 means: each row has a 10% chance to be included.\n",
        "# withReplacement : Allow duplicates in sample?\n",
        "# False (default) ‚Äî Each selected row appears at most once. choosing without putting the item back.\n",
        "# True ‚Äî Rows can be selected multiple times. behaves like picking an item, then putting it back, and picking again.\n",
        "# seed : Controls randomness to make result repeatable\n",
        "df.sample(fraction=0.1, seed=42)\n",
        "df.sample(withReplacement=False, fraction=0.2)\n",
        "\n",
        "# randomSplit(weights, seed=None) Randomly splits this DataFrame with the provided weights.\n",
        "# Split DataFrame into training (70%) and testing (30%)\n",
        "train_df, test_df = df.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "# ============================================================================\n",
        "# COLUMN OPERATIONS\n",
        "# ============================================================================\n",
        "\n",
        "# DataFrame.columns - Retrieves the names of all columns in the DataFrame as a list.\n",
        "selected_cols = [col for col in df.columns if col != \"age\"]\n",
        "df.select(selected_cols).show()\n",
        "# +-----+-----+\n",
        "# | name|state|\n",
        "# +-----+-----+\n",
        "# |  Tom|   CA|\n",
        "# |Alice|   NY|\n",
        "# |  Bob|   TX|\n",
        "# +-----+-----+\n",
        "\n",
        "# Conditional expressions\n",
        "from pyspark.sql.functions import when\n",
        "df.withColumn(\"category\",\n",
        "    when(col(\"age\") < 18, \"minor\")\n",
        "    .when((col(\"age\") >= 18) & (col(\"age\") < 65), \"adult\")\n",
        "    .otherwise(\"senior\")\n",
        ")\n",
        "\n",
        "# Null handling\n",
        "from pyspark.sql.functions import coalesce\n",
        "# ifnull() and nvl() return the second argument if the first is NULL, making them functionally identical.\n",
        "# nullif() does not replace NULL values. It returns NULL if two values are equal.\n",
        "# nvl2() does not compare values; it evaluates whether the first value is NULL or not and returns one of two provided options. IFNULL (ISNULL,YES,NO)\n",
        "\n",
        "df.withColumn(\"col_filled\", coalesce(col(\"col1\"), col(\"col2\"), lit(\"default\")))\n",
        "df.filter(col(\"col1\").isNull())       # Both are present inside functions, but we can skip import\n",
        "df.filter(col(\"col1\").isNotNull())    # as we are applying on col() which is imported specifically\n",
        "df.na.drop()  # Drop rows with any null\n",
        "df.na.drop(subset=[\"col1\", \"col2\"])  # Drop rows with null in specific columns\n",
        "df.na.fill(0)  # Fill all nulls with 0\n",
        "df.na.fill({\"col1\": 0, \"col2\": \"unknown\"})  # Fill specific columns\n",
        "\n",
        "# String operations\n",
        "from pyspark.sql.functions import upper, lower, trim, length, substring, concat_ws, split\n",
        "df.withColumn(\"upper_name\", upper(col(\"name\")))\n",
        "df.withColumn(\"lower_name\", lower(col(\"name\")))\n",
        "df.withColumn(\"trimmed\", trim(col(\"name\")))\n",
        "df.withColumn(\"name_length\", length(col(\"name\"))) # length of column\n",
        "df.withColumn(\"first_3_chars\", substring(col(\"name\"), 1, 3))\n",
        "df.withColumn(\"concatenated\", concat_ws(\"-\", col(\"col1\"), col(\"col2\")))\n",
        "\n",
        "# str : Column or column name to split\n",
        "# pattern : Column or literal string representing a regular expression should be a Java regular expression.\n",
        "# limit : Column or column name or int which controls the number of times pattern is applied.\n",
        "# limit > 0 : The resulting array‚Äôs length will not be more than limit, and the resulting array‚Äôs last entry will contain all input beyond the last matched pattern.\n",
        "# limit <= 0 (default): pattern will be applied as many times as possible, and the resulting array can be of any size.\n",
        "df.withColumn(\"split_array\", split(col(\"address\"), \",\"))\n",
        "df.withColumn(\"first_element\", split(col(\"address\"), \",\").getItem(0))\n",
        "\n",
        "# Date operations\n",
        "from pyspark.sql.functions import to_date, to_timestamp, year, month, dayofmonth, weekofyear, typeof, date_format, date_sub, from_unixtime, to_utc_timestamp, from_utc_timestamp, convert_timezone\n",
        "df.withColumn(\"date_col\", to_date(col(\"string_date\"), \"dd-MM-yyyy\")) # give a format which is compatible with java timestamp class #Converts to a date\n",
        "df.withColumn(\"timestamp_col\", to_timestamp(col(\"string_timestamp\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
        "df.withColumn(\"year\", year(col(\"date_col\")))\n",
        "df.withColumn(\"month\", month(col(\"date_col\")))\n",
        "df.withColumn(\"day\", dayofmonth(col(\"date_col\")))\n",
        "df.withColumn(\"week_number\", weekofyear(col(\"date_col\")))\n",
        "df.select(typeof('dt'), date_format('dt', 'yy--MM--dd')) # date | 15--04--08 # Converts a date/timestamp/string to a value of string in the format specified by the date format https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
        "df.withColumn(\"date_plus_7\", date_sub(df.dt, 1)) # Returns the date that is days days before start. If days is a negative value then these amount of days will be added to start.\n",
        "# from_unixtime(timestamp, format='yyyy-MM-dd HH:mm:ss') format is in this format https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
        "df.withColumn(\"timestamp_from_epoch\", from_unixtime('unix_time')) # 1428476400 -> 2015-04-08 00:00:00\n",
        "df.withColumn(\"unix_time\", unix_timestamp('timestamp_from_epoch')) # 2015-04-08 00:00:00 -> 1428476400\n",
        "df.withColumn(\"utc_timestamp\", to_utc_timestamp(col(\"datetime_utc\"), \"Asia/Tokyo\") ) # converts a timestamp in a given time zone into UTC.\n",
        "df.withColumn(\"Tokyo_timestamp\", from_utc_timestamp(col(\"datetime_utc\"), \"Asia/Tokyo\")) # converts a timestamp from UTC into the specified time zone.\n",
        "df.withColumn('HK_2_LA_timestamp', sf.convert_timezone(sf.lit('Asia/Hong_Kong'), sf.lit('America/Los_Angeles'), df.ts)) # convert_timezone(sourceTz, targetTz, sourceTs)Converts the timestamp without time zone sourceTs from the sourceTz time zone to targetTz.\n",
        "\n",
        "# Cast / Type conversion\n",
        "from pyspark.sql.types import IntegerType, DoubleType\n",
        "df.withColumn(\"col_as_int\", col(\"col1\").cast(\"int\"))\n",
        "df.withColumn(\"col_as_int2\", col(\"col1\").cast(IntegerType()))\n",
        "df.withColumn(\"col_as_double\", col(\"col1\").cast(DoubleType()))\n",
        "\n",
        "# Array and Map operations NOTE ALL ARE LOWERCASE INSIDE FUNCTIONS\n",
        "from pyspark.sql.functions import flatten, array, size, struct, sort_array, collect_list, explode, array_union, array_append, create_map, map_concat, map_keys, array_contains\n",
        "\n",
        "# creates a single array from an array of arrays. If a structure of nested arrays is deeper than two levels, only one level of nesting is removed.\n",
        "df.select(flatten(df.data)).show() # [None, [4, 5]], -> NULL   ||   [[1, 2, 3], [4, 5], [6]], -> [1, 2, 3, 4, 5, 6]\n",
        "df.withColumn(\"array_col\", array(col(\"col1\"), col(\"col2\"), col(\"col3\"))) # [col1_val, col2_val, col3_val] - For access df.select(col(\"array_col\")[0], col(\"array_col\")[1]).show()\n",
        "df.withColumn(\"size\", size(col(\"array_col\"))) # 3\n",
        "df.withColumn(\"struct_col\", struct(col(\"col1\"), col(\"col2\"), col(\"col3\"))) # {col1_val, col2_val, col3_val} - For access df.select(\"struct_col.col1\", \"struct_col.col2\").show()\n",
        "df.withColumn(\"address\", struct(col(\"address.city\"), col(\"address.zip\").alias(\"zipcode\"))) # renames zip to zipcode\n",
        "\n",
        "# Struct column\n",
        "df.select(col(\"address.city\")).show()\n",
        "df.select(\"address.city\").show()\n",
        "df.select(col(\"address\")[\"city\"]).show()\n",
        "\n",
        "# AS PER AI FIRST LINE IS INCORRECT LAST LINE IS CORRECT - Adds a new line\n",
        "df.withColumn(\"struct_col.col4\", lit(4)) # works as struct_col is already a struct, but if it was map then use map_concat, if it was array use array_append\n",
        "df.withColumn(\"address\", struct(col(\"address.*\"), lit(\"country\").alias(\"country\")))\n",
        "\n",
        "# sort_array(col, asc=True) - Sorts the input array in asc or desc order according to natural ordering of the array elements.\n",
        "# Null elements will be placed at the beginning of the returned array in ascending order or at the end of the returned array in descending order.\n",
        "# collect_list(col) - Collects the values from a column into a list, maintaining duplicates, and returns this list of objects.\n",
        "df = spark.createDataFrame([(2,), (5,), (5,)], ('age',))\n",
        "df.select(sort_array(collect_list('age'), asc=False).alias('sorted_list')).show()\n",
        "\n",
        "df.withColumn(\"exploded\", explode(col(\"array_col\")).alias(\"item\"))\n",
        "'''\n",
        "explode takes a column containing an array or map and turns each element into a separate row.\n",
        "  *  If the column is an array, you get one row per element.\n",
        "  *  If the column is a map, you get one row per key-value pair.\n",
        "\n",
        "# ARRAY\n",
        "data = [\n",
        "    (1, [\"apple\", \"banana\", \"orange\"]),\n",
        "    (2, [\"kiwi\"]),\n",
        "    (3, [])\n",
        "]\n",
        "df = spark.createDataFrame(data, [\"id\", \"fruits\"])\n",
        "df.show(truncate=False)\n",
        "\t+---+---------------------------+\n",
        "\t|id |fruits                     |\n",
        "\t+---+---------------------------+\n",
        "\t|1  |[apple, banana, orange]    |\n",
        "\t|2  |[kiwi]                     |\n",
        "\t|3  |[]                         |\n",
        "\t+---+---------------------------+\n",
        "\n",
        "df2 = df.withColumn(\"fruit\", explode(\"fruits\"))\n",
        "df2.show()\n",
        "\t+---+---------------------------+------+\n",
        "\t|id |fruits                     |fruit |\n",
        "\t+---+---------------------------+------+\n",
        "\t|1  |[apple, banana, orange]    |apple |\n",
        "\t|1  |[apple, banana, orange]    |banana|\n",
        "\t|1  |[apple, banana, orange]    |orange|\n",
        "\t|2  |[kiwi]                     |kiwi  |\n",
        "\t+---+---------------------------+------+\n",
        "\n",
        "# MAP\n",
        "data = [\n",
        "    (1, {\"a\": 10, \"b\": 20}),\n",
        "    (2, {\"x\": 5})\n",
        "]\n",
        "df = spark.createDataFrame(data, [\"id\", \"scores\"])\n",
        "df2 = df.withColumn(\"kv\", explode(\"scores\"))\n",
        "df2.show()\n",
        "  +---+-----------+-----+\n",
        "  |id |scores     |kv   |\n",
        "  +---+-----------+-----+\n",
        "  |1  |{a -> 10...}|[a,10]|\n",
        "  |1  |{a -> 10...}|[b,20]|\n",
        "  |2  |{x -> 5}   |[x,5] |\n",
        "  +---+-----------+-----+\n",
        "\n",
        "Then you can split keys and values using:\n",
        "# df3 = df2.withColumn(\"key\", col(\"kv\").getItem(0)) \\\n",
        "#          .withColumn(\"value\", col(\"kv\").getItem(1))\n",
        "# df3.show()\n",
        "\n",
        "posexplode also gives the position (index):\n",
        "# df.withColumn(\"pos_value\", posexplode(\"fruits\")).show()\n",
        "\n",
        "explode_outer example (keeps NULL and empty arrays)\n",
        "'''\n",
        "df.withColumn(\"first_element\", col(\"array_col\")[0])\n",
        "\n",
        "# array_union(col1, col2) - returns a new array containing the union of elements in col1 and col2, without duplicates.\n",
        "df.select(array_union(df.c1, df.c2)).show()\n",
        "df_with_new_fruit = df.withColumn(\"fruits\", array_append(df[\"fruits\"], \"kiwi\"))\n",
        "'''\n",
        "+---+---------------------+           +---+-------------------------+\n",
        "|id |fruits               |           |id |fruits                   |\n",
        "+---+---------------------+           +---+-------------------------+\n",
        "|1  |[apple, banana]      |           |1  |[apple, banana, kiwi]    |\n",
        "|2  |[orange, grape]      |           |2  |[orange, grape, kiwi]    |\n",
        "+---+---------------------+           +---+-------------------------+\n",
        "'''\n",
        "# element_at(col, extraction)\n",
        "# Returns element of array at given (1-based) index. If Index is 0, Spark will throw an error. If index < 0, accesses elements from the last to the first.\n",
        "df.select(element_at(col(\"numbers\"), 1).alias(\"first_element\")).show()\n",
        "\n",
        "# create_map(*cols): Creates a new map column from an even number of input columns or column references.\n",
        "# map_concat(*cols): Returns the union of all given maps.\n",
        "df.withColumn(\"attributes\", map_concat(col(\"attributes\"), create_map(lit(\"C\"), lit(3))))\n",
        "'''\n",
        "+---+----------------+            +---+------------------------+\n",
        "| id|      attributes|            |id |attributes              |\n",
        "+---+----------------+            +---+------------------------+\n",
        "|  1|{A -> 1, B -> 2}|            |1  |{A -> 1, B -> 2, C -> 3}|\n",
        "|  2|{A -> 2, B -> 3}|            |2  |{A -> 2, B -> 3, C -> 3}|\n",
        "|  3|{A -> 3, B -> 4}|            |3  |{A -> 3, B -> 4, C -> 3}|\n",
        "+---+----------------+            +---+------------------------+\n",
        "'''\n",
        "# Returns an unordered array containing the keys of the map.\n",
        "df.select(map_keys(col(\"attributes\")).alias(\"keys\")).show()\n",
        "\n",
        "# Checks if value exists in array\n",
        "df.filter(array_contains(col(\"skills\"), \"Python\"))\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# AGGREGATIONS\n",
        "# ============================================================================\n",
        "\n",
        "from pyspark.sql.functions import count, sum, avg, min, max, countDistinct, round, expr, approx_count_distinct, percentile_approx\n",
        "\n",
        "# Simple aggregations: WIDE TRANSFORMATION\n",
        "df.select(\n",
        "    count(\"*\").alias(\"total_count\"),\n",
        "    sum(\"Quantity\").alias(\"total_quantity\"),\n",
        "    avg(\"UnitPrice\").alias(\"avg_price\"),\n",
        "    min(\"UnitPrice\").alias(\"min_price\"),\n",
        "    max(\"UnitPrice\").alias(\"max_price\"),\n",
        "    countDistinct(\"InvoiceNo\").alias(\"distinct_invoices\")\n",
        "    approx_count_distinct(\"InvoiceNo\").alias(\"approx_distinct_invoices\"),\n",
        "    percentile_approx(col, 0.95, 10000) # percentile_approx(col, percentage, accuracy=10000)\n",
        ").show()\n",
        "\n",
        "# Aggregate on the entire DataFrame without groups (shorthand for df.groupBy().agg())\n",
        "product_df.agg(F.max(\"price\").alias(\"max_price\"), F.sum(\"price\")).show()\n",
        "\n",
        "df.groupBy(\"department\").sum(\"salary\").avg(\"salary\") # also valid but avg is calculated on sum\n",
        "product_df.agg({\"age\": \"max\"}).show()\n",
        "\n",
        "# GroupBy : WIDE TRANSFORMATION\n",
        "# returns a groupedData (variant of a dataframe) object instead of a dataframe\n",
        "# GroupBy with aggregations\n",
        "summary_df = df.groupBy(\"Country\", \"InvoiceNo\") \\\n",
        "    .agg(\n",
        "        sum(\"Quantity\").alias(\"TotalQuantity\"),\n",
        "        round(sum(expr(\"Quantity * UnitPrice\")), 2).alias(\"InvoiceValue\"),\n",
        "        count(\"*\").alias(\"num_items\"),\n",
        "        avg(\"UnitPrice\").alias(\"avg_price\")\n",
        "    )\n",
        "\n",
        "df.groupBy().sum().collect()[0][0]\n",
        "# groupBy() with no columns creates a global aggregation.\n",
        "# .sum() computes the sum of the ‚ÄúNumber‚Äù column.\n",
        "# .collect() returns a list of Row objects.\n",
        "# [0][0] extracts the aggregated value from the first row and first column.\n",
        "\n",
        "# Multiple aggregations defined separately\n",
        "num_invoices = countDistinct(\"InvoiceNo\").alias(\"NumInvoices\")\n",
        "total_quantity = sum(\"Quantity\").alias(\"TotalQuantity\")\n",
        "invoice_value = expr(\"round(sum(Quantity * UnitPrice), 2) as InvoiceValue\")\n",
        "\n",
        "agg_df = df \\\n",
        "    .withColumn(\"InvoiceDate\", to_date(col(\"InvoiceDate\"), \"dd-MM-yyyy H.mm\")) \\\n",
        "    .where(\"year(InvoiceDate) == 2010\") \\\n",
        "    .withColumn(\"WeekNumber\", weekofyear(col(\"InvoiceDate\"))) \\\n",
        "    .groupBy(\"Country\", \"WeekNumber\") \\\n",
        "    .agg(num_invoices, total_quantity, invoice_value)\n",
        "\n",
        "# Count by group (shorthand)\n",
        "df.groupBy(\"CallType\").count().orderBy(\"count\", ascending=False)\n",
        "\n",
        "# Pivot\n",
        "df.groupBy(\"Country\").pivot(\"Year\").sum(\"Revenue\")\n",
        "\n",
        "# ============================================================================\n",
        "# WINDOW FUNCTIONS (Value per window)\n",
        "# ============================================================================\n",
        "\n",
        "from pyspark.sql import Window\n",
        "from pyspark.sql.functions import sum, avg, row_number, rank, dense_rank, lag, lead\n",
        "\n",
        "# Window.partitionBy : WIDE TRANSFORMATION\n",
        "# Define window specifications\n",
        "window_spec = Window.partitionBy(\"Country\").orderBy(\"WeekNumber\")\n",
        "\n",
        "running_total_window = Window \\\n",
        "    .partitionBy(\"Country\") \\\n",
        "    .orderBy(\"WeekNumber\") \\\n",
        "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
        "\n",
        "# Unbounded window (entire partition)\n",
        "unbounded_window = Window.partitionBy(\"Country\")\n",
        "\n",
        "# Range between (value-based)\n",
        "range_window = Window \\\n",
        "    .partitionBy(\"product\") \\\n",
        "    .orderBy(\"date\") \\\n",
        "    .rangeBetween(-7, 0)  # Last 7 days\n",
        "\n",
        "# Apply window functions\n",
        "df_with_windows = df.withColumn(\"RunningTotal\", sum(\"InvoiceValue\").over(running_total_window))\n",
        "\n",
        "df_with_windows = df.withColumn(\"row_number\", row_number().over(window_spec))\n",
        "\n",
        "df_with_windows = df.withColumn(\"rank\", rank().over(window_spec))\n",
        "\n",
        "df_with_windows = df.withColumn(\"dense_rank\", dense_rank().over(window_spec))\n",
        "\n",
        "df_with_windows = df.withColumn(\"lag_value\", lag(\"value\", 1).over(window_spec))\n",
        "\n",
        "df_with_windows = df.withColumn(\"lead_value\", lead(\"value\", 1).over(window_spec))\n",
        "\n",
        "df_with_windows = df.withColumn(\"cumulative_sum\", sum(\"amount\").over(running_total_window))\n",
        "\n",
        "df_with_windows = df.withColumn(\"avg_per_country\", avg(\"amount\").over(unbounded_window))\n",
        "# Step 1: aggregate per country\n",
        "# country_avg_df = df.groupBy(\"Country\").agg(avg(\"amount\").alias(\"avg_per_country\"))\n",
        "# Step 2: join back to original DataFrame\n",
        "# df_without_window = df.join(country_avg_df, on=\"Country\", how=\"left\")\n",
        "\n",
        "# ============================================================================\n",
        "# UNION AND INTERSECTION\n",
        "# ============================================================================\n",
        "\n",
        "# ALL WIDE TRANSFORMATIONS\n",
        "# Union (combines DataFrames, allows duplicates)\n",
        "union_df = df1.union(df2)\n",
        "union_df = df1.unionAll(df2)  # Deprecated, use union()\n",
        "\n",
        "# Union by name (matches columns by name, not position)\n",
        "union_df = df1.unionByName(df2)\n",
        "union_df = df1.unionByName(df2, allowMissingColumns=True)\n",
        "\n",
        "# Intersection (common rows) duplicates are removed. To preserve duplicates use intersectAll().\n",
        "intersection_df = df1.intersect(df2)\n",
        "\n",
        "# Except / Subtract (rows in df1 but not in df2)\n",
        "except_df = df1.subtract(df2) #SQL Except - set(df1) - set(df2)\n",
        "except_df = df1.exceptAll(df2)  # Keeps duplicates # SQL Except all - If a row appears n times in df1 and m times in df2, output will contain it max(n ‚àí m, 0) times.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeB07gVn2MQM"
      },
      "source": [
        "---\n",
        "**Action**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZDSVyFeFDx4"
      },
      "outputs": [],
      "source": [
        "# collect(): This action returns all the rows of the DataFrame as an array. It is useful for debugging purposes, but should be used with caution, as it may cause out-of-memory errors if the DataFrame is large.\n",
        "rows = df.collect()\n",
        "\n",
        "# count(): This action returns the number of rows in the DataFrame.\n",
        "row_count = df.count()\n",
        "\n",
        "# first(): This action returns the first row of the DataFrame.\n",
        "first_row = df.first()\n",
        "\n",
        "# head(n = None): This action returns the first n rows if avilable of the DataFrame. At most n rows. 1 row if n is not passed\n",
        "# This method should only be used if the resulting array is expected to be small, as all the data is loaded into the driver‚Äôs memory.\n",
        "# here n is optional & head() and first() are same\n",
        "# head may return a row object if n = 1 or list of row object when n > 1\n",
        "head_rows = df.head(5)\n",
        "\n",
        "# take(n): This action returns the first n rows of the DataFrame. Always returns a list of Row objects\n",
        "take_rows = df.take(10)\n",
        "\n",
        "# df.show(n=20, truncate=True, vertical=False) https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.show.html\n",
        "# n: Number of rows to show.\n",
        "# truncate: bool or int, optional, default True | If set to True, truncate strings longer than 20 chars. If set to a number greater than one, truncates long strings to length truncate and align cells right.\n",
        "# vertical bool, optional | If set to True, print output rows vertically (one line per column value).\n",
        "df.show()\n",
        "df.show(20, truncate=False)\n",
        "df.show(n=10, vertical=True)\n",
        "\n",
        "# foreach(func): This action applies a function func to each row of the DataFrame. It is useful for performing an action on each row of the DataFrame, such as saving the row to a database. or modifying an accumulator\n",
        "df = spark.createDataFrame( [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"] )\n",
        "def func(person):\n",
        "    print(person.name)\n",
        "df.foreach(func)\n",
        "\n",
        "# reduce(func): This action reduces the rows of the RDD to a single value by applying a function func that takes two rows and returns a single row.\n",
        "# ((((row1 + row2) + row3) + row4) ... )\n",
        "df = spark.createDataFrame([\n",
        "    (1, \"A\"),\n",
        "    (2, \"B\"),\n",
        "    (3, \"C\")\n",
        "], [\"value\", \"label\"])\n",
        "\n",
        "def add_rows(r1, r2):\n",
        "    return Row(value=r1.value + r2.value)\n",
        "\n",
        "result = df.select(\"value\").rdd.reduce(add_rows)\n",
        "print(result) # Row(value=6)\n",
        "\n",
        "# Returns an iterator that contains all of the rows in this DataFrame. The iterator will consume as much memory as the largest partition in this DataFrame.\n",
        "# With prefetch it may consume up to the memory of the 2 largest partitions.\n",
        "df = spark.createDataFrame( [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"] )\n",
        "list(df.toLocalIterator())\n",
        "# [Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\n",
        "\n",
        "# Convert to Pandas, Collects entire DF to driver as Pandas ‚Üí heavy, risky.\n",
        "pandas_df = df.toPandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnqVykNL2iPe"
      },
      "source": [
        "---\n",
        "**Utility Methods**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qc65QPXe61m0"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 15. CACHING AND PERSISTENCE\n",
        "# ============================================================================\n",
        "\n",
        "# Cache in memory\n",
        "df.cache() #shortcut for df.persist(StorageLevel.) MEMORY_AND_DISK_DESER\n",
        "df.persist()  # Same as cache()\n",
        "# Note: an action like df.count() is needed for it to cache, if used df.take(n) Only caches the partition(s) needed to fetch n row ‚Üí NOT full cache.\n",
        "\n",
        "# Persist with storage level\n",
        "from pyspark import StorageLevel\n",
        "# pyspark.StorageLevel(useDisk, useMemory, useOffHeap, deserialized, replication=1)\n",
        "# StorageLevel.MEMORY_AND_DISK_2 is Same as MEMORY_AND_DISK storage level but replicate each partition to two cluster nodes.\n",
        "df.persist(StorageLevel.MEMORY_AND_DISK_DESER) # default\n",
        "df.persist(StorageLevel.MEMORY_AND_DISK)\n",
        "df.persist(StorageLevel.DISK_ONLY)\n",
        "df.persist(StorageLevel.MEMORY_ONLY)\n",
        "'''\n",
        "| StorageLevel              | useDisk | useMemory | useOffHeap | deserialized | replication |\n",
        "| ------------------------- | :-----: | :-------: | :--------: | :----------: | :---------: |\n",
        "|   NONE                    |  False  |   False   |    False   |     False    |      0      |\n",
        "|   DISK_ONLY               |   True  |   False   |    False   |     False    |      1      |\n",
        "|   DISK_ONLY_2             |   True  |   False   |    False   |     False    |      2      |\n",
        "|   DISK_ONLY_3             |   True  |   False   |    False   |     False    |      3      |\n",
        "|   MEMORY_ONLY             |  False  |    True   |    False   |     False    |      1      |\n",
        "|   MEMORY_ONLY_2           |  False  |    True   |    False   |     False    |      2      |\n",
        "|   MEMORY_AND_DISK         |   True  |    True   |    False   |     False    |      1      |\n",
        "|   MEMORY_AND_DISK_2       |   True  |    True   |    False   |     False    |      2      |\n",
        "|   MEMORY_AND_DISK_DESER   |   True  |    True   |    False   |     True     |      1      |\n",
        "|   OFF_HEAP                |  True   |    True   |    True    |     False    |      1      |\n",
        "'''\n",
        "\n",
        "# Unpersist\n",
        "df.unpersist()\n",
        "\n",
        "# Check if cached\n",
        "df.is_cached\n",
        "\n",
        "# ============================================================================\n",
        "# 16. OTHER UTILITY FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "# Prints out the schema in the tree format. Optionally allows to specify how many levels to print if schema is nested.\n",
        "df.printSchema()\n",
        "# root\n",
        "#  |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
        "#  |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
        "#  |-- count: long (nullable = false)\n",
        "\n",
        "df.schema.simpleString()\n",
        "# struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:bigint>\n",
        "\n",
        "# Returns the schema of this DataFrame as a pyspark.sql.types.StructType.\n",
        "schema = df.schema\n",
        "# StructType([StructField('age', LongType(), True), StructField('name', StringType(), True)])\n",
        "\n",
        "# Retrieves the names of all columns in the DataFrame as a list. The order of the column names in the list reflects their order in the DataFrame.\n",
        "columns = df.columns\n",
        "# ['age', 'name', 'state']\n",
        "\n",
        "# Returns all column names and their data types as a list.\n",
        "dtypes = df.dtypes\n",
        "#[('age', 'bigint'), ('name', 'string')]\n",
        "\n",
        "# DataFrame.describe(*cols)\t\tComputes basic statistics for numeric and string columns.\n",
        "# This includes count, mean, stddev, min, and max. If no columns are given, this function computes statistics for all numerical or string columns.\n",
        "# Returns A new DataFrame that describes (provides statistics) given DataFrame.\n",
        "df.describe(['age', 'weight', 'height']).show()\n",
        "# +-------+----+------------------+-----------------+\n",
        "# |summary| age|            weight|           height|\n",
        "# +-------+----+------------------+-----------------+\n",
        "# |  count|   3|                 3|                3|\n",
        "# |   mean|12.0| 40.73333333333333|            145.0|\n",
        "# | stddev| 1.0|3.1722757341273704|4.763402145525822|\n",
        "# |    min|  11|              37.8|            142.2|\n",
        "# |    max|  13|              44.1|            150.5|\n",
        "# +-------+----+------------------+-----------------+\n",
        "\n",
        "# DataFrame.summary(*statistics)    Computes specified statistics for numeric and string columns.\n",
        "# Available statistics are: - count - mean - stddev - min - max - arbitrary approximate percentiles specified as a percentage (e.g., 75%)\n",
        "df.select(\"age\", \"weight\", \"height\").summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show()\n",
        "# +-------+---+------+------+\n",
        "# |summary|age|weight|height|\n",
        "# +-------+---+------+------+\n",
        "# |  count|  3|     3|     3|\n",
        "# |    min| 11|  37.8| 142.2|\n",
        "# |    25%| 11|  37.8| 142.2|\n",
        "# |    75%| 13|  44.1| 150.5|\n",
        "# |    max| 13|  44.1| 150.5|\n",
        "# +-------+---+------+------+\n",
        "\n",
        "\n",
        "# DataFrame.explain(extended=None, mode=None)\t\tPrints the (logical and physical) plans to the console for debugging purposes.\n",
        "#\n",
        "# extended (bool, default = False)\n",
        "#   False ‚Üí prints only the physical plan\n",
        "#   If a string is passed, it‚Äôs treated as the mode\n",
        "#\n",
        "# mode (str) ‚Äì output format:\n",
        "#   simple ‚Üí physical plan only\n",
        "#   extended ‚Üí logical + physical plans\n",
        "#   codegen ‚Üí physical plan + generated code (if available)\n",
        "#   cost ‚Üí logical plan + statistics (if available)\n",
        "#   formatted ‚Üí physical plan outline + detailed node info\n",
        "df.explain(extended=True)\n",
        "\n",
        "# The internal Java DataFrame backing your PySpark DataFrame\n",
        "# \t_jdf = Java DataFrame\n",
        "# \tExposes Spark‚Äôs JVM internals via Py4J\n",
        "# \tNot part of the public API (meant for debugging / internals)\n",
        "df._jdf.queryExecution().toString()\n",
        "\n",
        "# BOTH WILL RESULT IN\n",
        "\n",
        "# == Parsed Logical Plan ==\n",
        "# ...\n",
        "# == Analyzed Logical Plan ==\n",
        "# ...\n",
        "# == Optimized Logical Plan ==\n",
        "# ...\n",
        "# == Physical Plan ==\n",
        "# ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Su12AReWBr2"
      },
      "source": [
        "---\n",
        "**SALTING** (Joins + Aggregation https://chatgpt.com/c/6947f284-197c-8321-a6a2-41eefded3822)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJU9_NG9c2rg"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
        "import random\n",
        "\n",
        "\"\"\"\n",
        "Salting is a technique to handle data skew by:\n",
        "1. Adding a random \"salt\" value to skewed keys\n",
        "2. Distributing hot keys across multiple partitions\n",
        "3. Processing in parallel instead of single partition\n",
        "4. Aggregating results after processing\n",
        "\n",
        "USES:\n",
        "- Handling skewed joins\n",
        "- Fixing aggregation bottlenecks\n",
        "- Improving shuffle performance\n",
        "- Balancing partition sizes\n",
        "\"\"\"\n",
        "# ============================================================================\n",
        "# 1. DATA SKEW - THE PROBLEM\n",
        "# ============================================================================\n",
        "\n",
        "# Example: Creating skewed data\n",
        "skewed_data = []\n",
        "# Create heavy skew - 80% records have same key\n",
        "for i in range(8000):\n",
        "    skewed_data.append((\"hot_key\", f\"value_{i}\", i))\n",
        "# Non skewed data\n",
        "for i in range(2000):\n",
        "    skewed_data.append((f\"key_{i}\", f\"value_{i}\", i))\n",
        "\n",
        "skewed_df = spark.createDataFrame(skewed_data, [\"key\", \"value\", \"amount\"])\n",
        "# Number of partitions would be value mentioned here: spark.default.parallelism\n",
        "# Post any wide transformation it would follow: spark.sql.shuffle.partitions\n",
        "\n",
        "# ============================================================================\n",
        "# 2. SALTING TECHNIQUE #1: Simple Random Salting\n",
        "# ============================================================================\n",
        "\"\"\"\n",
        "Use Case: When you have skewed aggregations\n",
        "Adds random suffix to distribute keys across partitions\n",
        "\"\"\"\n",
        "\n",
        "# rand(seed=None) - Generates a random column with independent and identically distributed (i.i.d.) samples uniformly distributed in [0.0, 1.0).\n",
        "# seed - Seed value for the random generator.\n",
        "\n",
        "def add_salt(df, key_col, salt_range=10):\n",
        "    \"\"\"Add random salt to a key column\"\"\"\n",
        "    return df.withColumn(\n",
        "        \"salt\",\n",
        "        F.lit(F.floor(F.rand() * salt_range).cast(\"int\"))\n",
        "    ).withColumn(\n",
        "        f\"{key_col}_salted\",\n",
        "        F.concat(F.col(key_col), F.lit(\"_\"), F.col(\"salt\"))\n",
        "    )\n",
        "\n",
        "# Apply salting\n",
        "salted_df = add_salt(skewed_df, \"key\", salt_range=10)\n",
        "\n",
        "# Perform aggregation on salted keys\n",
        "result = salted_df.groupBy(\"key_salted\").agg(\n",
        "    F.sum(\"amount\").alias(\"total_amount\")\n",
        ")\n",
        "\n",
        "# Remove salt and re-aggregate\n",
        "final_result = result.withColumn(\n",
        "    \"original_key\",\n",
        "    F.regexp_replace(F.col(\"key_salted\"), \"_\\\\d+$\", \"\")\n",
        ").groupBy(\"original_key\").agg(\n",
        "    F.sum(\"total_amount\").alias(\"final_total\")\n",
        ")\n",
        "\n",
        "final_result.show()\n",
        "\n",
        "# ============================================================================\n",
        "# 2. SALTING TECHNIQUE #2: Salted Join\n",
        "# ============================================================================\n",
        "\"\"\"\n",
        "Use Case: When joining tables with skewed keys\n",
        "Expands smaller table and salts larger table for balanced join\n",
        "\"\"\"\n",
        "\n",
        "def salted_join(large_df, small_df, join_key, salt_range=10):\n",
        "    \"\"\"\n",
        "    Perform a salted join between skewed and normal DataFrames\n",
        "    Steps:\n",
        "    1. Explode small table by replicating with all salt values\n",
        "    2. Add random salt to large table\n",
        "    3. Join on salted keys\n",
        "    4. Remove salt columns\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Explode small table\n",
        "    salt_values = list(range(salt_range))\n",
        "    small_exploded = small_df.withColumn(\n",
        "        \"salt\",\n",
        "        F.explode(F.array([F.lit(i) for i in salt_values]))\n",
        "    ).withColumn(\n",
        "        f\"{join_key}_salted\",\n",
        "        F.concat(F.col(join_key), F.lit(\"_\"), F.col(\"salt\"))\n",
        "    )\n",
        "\n",
        "    # Step 2: Salt large table\n",
        "    large_salted = add_salt(large_df, join_key, salt_range)\n",
        "\n",
        "    # Step 3: Join\n",
        "    joined = large_salted.join(\n",
        "        small_exploded,\n",
        "        large_salted[f\"{join_key}_salted\"] == small_exploded[f\"{join_key}_salted\"],\n",
        "        \"inner\"\n",
        "    )\n",
        "\n",
        "    # Step 4: Clean up\n",
        "    return joined.drop(\"salt\", f\"{join_key}_salted\")\n",
        "\n",
        "# Example: Salted Join\n",
        "orders = spark.createDataFrame([\n",
        "    (\"hot_key\", \"order1\", 100),\n",
        "    (\"hot_key\", \"order2\", 200),\n",
        "    (\"hot_key\", \"order3\", 150),\n",
        "    (\"key_1\", \"order4\", 300),\n",
        "], [\"customer_id\", \"order_id\", \"amount\"])\n",
        "\n",
        "customers = spark.createDataFrame([\n",
        "    (\"hot_key\", \"VIP Customer\"),\n",
        "    (\"key_1\", \"Regular Customer\"),\n",
        "], [\"customer_id\", \"customer_type\"])\n",
        "\n",
        "salted_join_result = salted_join(orders, customers, \"customer_id\", salt_range=3)\n",
        "salted_join_result.show()\n",
        "\n",
        "# ============================================================================\n",
        "# 3. SALTING TECHNIQUE #3: Adaptive Salting\n",
        "# ============================================================================\n",
        "\"\"\"\n",
        "Use Case: When only certain keys are skewed\n",
        "Selectively salt only the hot keys instead of entire dataset\n",
        "\"\"\"\n",
        "def adaptive_salting(df, key_col, threshold=1000, salt_range=10):\n",
        "    \"\"\"\n",
        "    Apply salting only to keys exceeding threshold\n",
        "    1. Identify hot keys\n",
        "    2. Salt only those keys\n",
        "    3. Union with non-hot keys\n",
        "    \"\"\"\n",
        "\n",
        "    # Identify hot keys\n",
        "    key_counts = df.groupBy(key_col).count()\n",
        "    hot_keys = key_counts.filter(F.col(\"count\") > threshold).select(key_col)\n",
        "\n",
        "    # Separate hot and cold data\n",
        "    hot_data = df.join(hot_keys, key_col, \"inner\")\n",
        "    cold_data = df.join(hot_keys, key_col, \"left_anti\")\n",
        "\n",
        "    # Salt hot data\n",
        "    hot_salted = add_salt(hot_data, key_col, salt_range)\n",
        "\n",
        "    # Process hot data\n",
        "    hot_result = hot_salted.groupBy(f\"{key_col}_salted\").agg(\n",
        "        F.sum(\"amount\").alias(\"total\")\n",
        "    ).withColumn(\n",
        "        key_col,\n",
        "        F.regexp_replace(F.col(f\"{key_col}_salted\"), \"_\\\\d+$\", \"\")\n",
        "    ).groupBy(key_col).agg(F.sum(\"total\").alias(\"amount\"))\n",
        "\n",
        "    # Process cold data\n",
        "    cold_result = cold_data.groupBy(key_col).agg(\n",
        "        F.sum(\"amount\").alias(\"amount\")\n",
        "    )\n",
        "\n",
        "    # Union results\n",
        "    return hot_result.union(cold_result)\n",
        "\n",
        "adaptive_result = adaptive_salting(skewed_df, \"key\", threshold=100, salt_range=5)\n",
        "adaptive_result.show()\n",
        "\n",
        "# ============================================================================\n",
        "# 4. SALTING TECHNIQUE #4: Two-Stage Aggregation (Same as Simple Random Salting up above)\n",
        "# ============================================================================\n",
        "\"\"\"\n",
        "Use Case: Heavy aggregations with multiple group-by columns\n",
        "Reduces data before final aggregation\n",
        "\"\"\"\n",
        "def two_stage_aggregation(df, group_cols, agg_col, salt_range=10):\n",
        "    \"\"\"\n",
        "    Two-stage aggregation with salting\n",
        "    Stage 1: Partial aggregation with salt\n",
        "    Stage 2: Final aggregation without salt\n",
        "    \"\"\"\n",
        "\n",
        "    # Stage 1: Add salt and partial aggregate\n",
        "    salted = df.withColumn(\"salt\", F.floor(F.rand() * salt_range).cast(\"int\"))\n",
        "\n",
        "    partial_agg = salted.groupBy(group_cols + [\"salt\"]).agg(\n",
        "        F.sum(agg_col).alias(\"partial_sum\"),\n",
        "        F.count(agg_col).alias(\"partial_count\")\n",
        "    )\n",
        "\n",
        "    # Stage 2: Final aggregation\n",
        "    final_agg = partial_agg.groupBy(group_cols).agg(\n",
        "        F.sum(\"partial_sum\").alias(\"total_sum\"),\n",
        "        F.sum(\"partial_count\").alias(\"total_count\")\n",
        "    )\n",
        "\n",
        "    return final_agg\n",
        "\n",
        "two_stage_result = two_stage_aggregation(skewed_df, [\"key\"], \"amount\", salt_range=5)\n",
        "two_stage_result.show()\n",
        "\n",
        "# ============================================================================\n",
        "# 9. MONITORING & DEBUGGING SKEW\n",
        "# ============================================================================\n",
        "\"\"\"\n",
        "Detecting Data Skew:\n",
        "-------------------\n",
        "\"\"\"\n",
        "\n",
        "def analyze_partition_distribution(df, key_col):\n",
        "    \"\"\"Analyze how data is distributed across partitions\"\"\"\n",
        "\n",
        "    # Add partition ID\n",
        "    with_partition = df.withColumn(\"partition_id\", F.spark_partition_id())\n",
        "\n",
        "    # Count records per partition\n",
        "    partition_stats = with_partition.groupBy(\"partition_id\").agg(\n",
        "        F.count(\"*\").alias(\"record_count\")\n",
        "    ).orderBy(\"record_count\", ascending=False)\n",
        "\n",
        "    print(\"\\n=== Partition Distribution ===\")\n",
        "    partition_stats.show()\n",
        "\n",
        "    # Key distribution\n",
        "    key_stats = df.groupBy(key_col).agg(\n",
        "        F.count(\"*\").alias(\"count\")\n",
        "    ).orderBy(\"count\", ascending=False)\n",
        "\n",
        "    print(\"\\n=== Top Keys by Count ===\")\n",
        "    key_stats.show(10)\n",
        "\n",
        "    return partition_stats, key_stats\n",
        "\n",
        "# Analyze skew\n",
        "analyze_partition_distribution(skewed_df, \"key\")\n",
        "\n",
        "\"\"\"\n",
        "SALTING BEST PRACTICES:\n",
        "1. When to Use Salting:\n",
        "   - Joins with skewed keys (>5% data in single key)\n",
        "   - Heavy aggregations on hot keys\n",
        "   - Tasks taking 3x+ longer than median\n",
        "2. Salt Range Selection:\n",
        "   - Small datasets: 5-10 salts\n",
        "   - Medium datasets: 10-50 salts\n",
        "   - Large datasets: 50-100 salts\n",
        "   - Rule: sqrt(num_executors) * 2\n",
        "3. Trade-offs:\n",
        "   - Pros: Better parallelism, prevents OOM\n",
        "   - Cons: Extra shuffle, increased data size\n",
        "4. Alternatives to Consider:\n",
        "   - Broadcast joins (for small tables)\n",
        "   - Bucketing (for repeated joins)\n",
        "   - Repartitioning with custom partitioner\n",
        "   - AQE (Adaptive Query Execution) in Spark 3.0+\n",
        "5. Monitoring:\n",
        "   - Use Spark UI to identify stragglers\n",
        "   - Check partition sizes in shuffle\n",
        "   - Monitor spill metrics\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epjjp8bjdP8M"
      },
      "source": [
        "---\n",
        "**COMMON PATTERNS AND BEST PRACTICES**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qlWHJsVdPmh"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, count, when\n",
        "\n",
        "# 1. Check data quality\n",
        "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
        "\n",
        "# 2. Find duplicates\n",
        "df.groupBy(df.columns).count().filter(col(\"count\") > 1).show()\n",
        "\n",
        "# 3. Sample data for testing\n",
        "sample_df = df.sample(fraction=0.01)\n",
        "\n",
        "# 4. Rename all columns (lowercase)\n",
        "df_clean = df.select([col(c).alias(c.lower()) for c in df.columns])\n",
        "\n",
        "# 5. Drop columns with all nulls\n",
        "non_null_counts = df.select([count(c).alias(c) for c in df.columns]).collect()[0]\n",
        "cols_to_keep = [c for c in df.columns if non_null_counts[c] > 0]\n",
        "df_filtered = df.select(cols_to_keep)\n",
        "\n",
        "# DataFrame.stat Returns a DataFrameStatFunctions for statistic functions.\n",
        "\n",
        "# approxQuantile(col, probabilities, relativeError)\n",
        "# Calculates the approximate quantiles of numerical columns of a DataFrame.\n",
        "df.stat.approxQuantile(\"price\", [0.25, 0.5, 0.75], 0.01)\n",
        "\n",
        "# corr(col1, col2[, method])\n",
        "# Calculates the correlation of two columns of a DataFrame as a double value.\n",
        "df.stat.corr(\"col1\", \"col2\")\n",
        "\n",
        "# cov(col1, col2)\n",
        "# Calculate the sample covariance for the given columns, specified by their names, as a double value.\n",
        "\n",
        "# crosstab(col1, col2)\n",
        "# Computes a pair-wise frequency table of the given columns.\n",
        "df.stat.crosstab(\"category\", \"status\").show()\n",
        "\n",
        "# freqItems(cols[, support])\n",
        "# Finding frequent items for columns, possibly with false positives.\n",
        "df.stat.freqItems([\"col1\", \"col2\"]).show()\n",
        "\n",
        "# sampleBy(col, fractions[, seed])\n",
        "# Returns a stratified sample without replacement based on the fraction given on each stratum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iJ2ailmVHxd"
      },
      "source": [
        "---\n",
        "**UDF**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zO3qtC3SVE9n"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# UDFs (User Defined Functions)\n",
        "# ============================================================================\n",
        "from pyspark.sql.functions import udf, expr\n",
        "\n",
        "# Define UDF\n",
        "def categorize_age(age):\n",
        "    if age < 18:\n",
        "        return \"minor\"\n",
        "    elif age < 65:\n",
        "        return \"adult\"\n",
        "    else:\n",
        "        return \"senior\"\n",
        "\n",
        "# Register UDF - Registered UDFs are serialized and sent to Spark executors.\n",
        "# Used when data transformation is done on Dataframe. Does not create a catalog entry. has local scope and cannot be used in SQL\n",
        "# udf(f=None, returnType=StringType(), *, useArrow=None)\n",
        "categorize_udf = udf(categorize_age, StringType())\n",
        "\n",
        "# Use UDF\n",
        "df_with_category = df.withColumn(\"category\", categorize_udf(col(\"age\")))\n",
        "\n",
        "# Register UDF for SQL - Adds the function to the Spark catalog for SQL usage. Hence, across all SQL queries in the session; however, neither method persists the UDF beyond the current SparkSession.\n",
        "# register(name, f, returnType=None)\n",
        "spark.udf.register(\"categorize_age_sql\", categorize_age, StringType())\n",
        "df.createOrReplaceTempView(\"people\")\n",
        "\n",
        "spark.sql(\"SELECT name, categorize_age_sql(age) as category FROM people\").show()\n",
        "df_with_category = df.withColumn(\"category\", expr(\"categorize_age_sql(age)\"))\n",
        "\n",
        "# Pandas UDF (vectorized, more efficient)\n",
        "from pyspark.sql.functions import pandas_udf\n",
        "import pandas as pd\n",
        "\n",
        "# Series to Series\n",
        "@pandas_udf(DoubleType())\n",
        "def multiply_by_two(col: pd.Series) -> pd.Series:\n",
        "    return col * 2\n",
        "\n",
        "df.withColumn(\"doubled\", multiply_by_two(col(\"value\"))).show()\n",
        "\n",
        "# Iterator of Series to Iterator of Series (for batching)\n",
        "@pandas_udf(\"double\")\n",
        "def multiply_batch(iterator):\n",
        "\tfor s in iterator:\n",
        "\t\tyield s * 2\n",
        "\n",
        "# Series to Scalar (aggregate)\n",
        "@pandas_udf(\"double\")\n",
        "def mean_udf(s: pd.Series) -> float:\n",
        "\treturn s.mean()\n",
        "\n",
        "df.groupBy(\"group\").agg(mean_udf(col(\"value\")))\n",
        "\n",
        "'''\n",
        "Types of Pandas UDFs:\n",
        "1. Series to Series: Element-wise transformations\n",
        "2. Iterator of Series to Iterator of Series: Batch processing\n",
        "3. Iterator of Multiple Series to Iterator of Series: Multiple input columns\n",
        "4. Series to Scalar: Aggregations\n",
        "5. Grouped Map: Operates on entire groups\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFEivZ2t5XV1"
      },
      "source": [
        "---\n",
        "**PANDAS API**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t43jjA3U5XDJ"
      },
      "outputs": [],
      "source": [
        "# Pandas-on-Spark is lazy once data is in Spark, but importing from Pandas is eager because the data already exists in memory.\n",
        "# Creating Pandas on Spark DataFrames:\n",
        "\n",
        "import pandas as pd\n",
        "import pyspark.pandas as ps\n",
        "\n",
        "# From Spark DataFrame\n",
        "psdf = spark_df.to_pandas_on_spark() # Lazy\n",
        "\n",
        "# From Pandas\n",
        "pdf = pd.DataFrame({'a': [1, 2, 3]})\n",
        "psdf = ps.from_pandas(pdf) # Spark job runs right away to parallelize the Pandas data, Subsequent operations on psdf are lazy, just like Spark\n",
        "\n",
        "# Direct creation Eager\n",
        "psdf = ps.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]})\n",
        "\n",
        "# Read files\n",
        "psdf = ps.read_csv(\"path/to/file.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFZaA8rjhnml"
      },
      "source": [
        "---\n",
        "**Accumulator**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BPxW4tSVFV1"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# ACCUMULATORS IN DATAFRAME UDFs\n",
        "# ============================================================================\n",
        "'''\n",
        "ACCUMULATORS:\n",
        "  ‚Ä¢ Write-only variables for aggregating metrics across executors on a per-row basis.\n",
        "  ‚Ä¢ Use for: counters, data quality tracking, error monitoring\n",
        "  ‚Ä¢ Only driver can read the final value.\n",
        "\n",
        "  ‚Ä¢ Be careful with transformations (may execute multiple times) Because transformations are lazy and may run multiple times: during job retries, during lineage recomputation, during stage re-execution\n",
        "  ‚Ä¢ For accumulator restarted tasks will not update the value in case of a failure. Accumulator update from the failed attempt is not counted. Spark only applies accumulator updates from successful task attempts.\n",
        "  ‚Ä¢ Use accumulators in actions or with cache/persist to avoid double-counting\n",
        "\n",
        "  ‚Ä¢ UI Visibility (Just name them):\n",
        "    ‚Ä¢ Scala: Named accumulators visible in Spark UI.\n",
        "    ‚Ä¢ PySpark: Unnamed; not displayed in UI.\n",
        "  ‚Ä¢ Types: Default support for Long/Float; custom accumulators possible (advanced, out of scope).\n",
        "    You can define your own custom accumulator class by extending org.apache.spark.util.AccumulatorV2 (V2) in Java or Scala or pyspark.AccumulatorParam in Python.\n",
        "'''\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "# Base DataFrame used for all versions\n",
        "df = spark.createDataFrame([\n",
        "    (10, 2), (20, 0), (15, 3), (8, 0), (12, 4)\n",
        "], [\"num\", \"denom\"])\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# ================ VERSION A: UNSAFE (Accumulator inside transformation, NO CACHE) ================\n",
        "error_counter = sc.accumulator(0)\n",
        "\n",
        "def safe_divide_udf_A(n, d):\n",
        "    if d == 0:\n",
        "        error_counter.add(1)        # Accumulator inside TRANSFORMATION ‚Üí may run multiple times\n",
        "        return None\n",
        "    return n / d\n",
        "\n",
        "safe_divide_A = udf(safe_divide_udf_A, StringType())\n",
        "\n",
        "df_A = df.withColumn(\"result\", safe_divide_A(col(\"num\"), col(\"denom\")))\n",
        "df_A.show()                         # Action triggers evaluation\n",
        "print(f\"Version A errors counted: {error_counter.value}\")  # May be double-counted depending on Spark plan\n",
        "\n",
        "# ================ VERSION B: SAFE (Accumulator used inside an ACTION) ================\n",
        "error_counter = sc.accumulator(0)\n",
        "\n",
        "def count_errors(row):\n",
        "    if row.denom == 0:\n",
        "        error_counter.add(1)        # Executed exactly once per row by foreach\n",
        "\n",
        "# Action executes the function exactly once\n",
        "df.foreach(count_errors)\n",
        "\n",
        "print(f\"Version B errors counted: {error_counter.value}\")\n",
        "\n",
        "# Compute results using a pure UDF (no side effects)\n",
        "safe_divide_B = udf(lambda n, d: None if d == 0 else n/d, StringType())\n",
        "df_B = df.withColumn(\"result\", safe_divide_B(col(\"num\"), col(\"denom\")))\n",
        "df_B.show()\n",
        "\n",
        "\n",
        "# ================ VERSION C: SAFE (Accumulator inside transformation, BUT cached before action) ================\n",
        "error_counter = sc.accumulator(0)\n",
        "\n",
        "def safe_divide_udf_C(n, d):\n",
        "    if d == 0:\n",
        "        error_counter.add(1)\n",
        "        return None\n",
        "    return n / d\n",
        "\n",
        "safe_divide_C = udf(safe_divide_udf_C, StringType())\n",
        "\n",
        "df_C = df.withColumn(\"result\", safe_divide_C(col(\"num\"), col(\"denom\")))\n",
        "\n",
        "df_C.cache()             # Key step to avoid multiple evaluations\n",
        "\n",
        "df_C.count()             # Triggers UDF once\n",
        "df_C.show()\n",
        "\n",
        "print(f\"Version C errors counted: {error_counter.value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qCQ73TWvPAo"
      },
      "source": [
        "---\n",
        "**Broadcast Variable**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9uD26cpvRgu"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# BROADCAST IN DATAFRAME TRANSFORMATIONS (Broadcast Join are also under this)\n",
        "# ============================================================================\n",
        "'''\n",
        "BROADCAST VARIABLES:\n",
        "  ‚Ä¢ Read-only variables cached on each executor in deserialized form (not sent per task)\n",
        "  ‚Ä¢ Use for: lookup tables, configuration, filtering lists\n",
        "  ‚Ä¢ Significantly reduces network overhead for large shared data\n",
        "  ‚Ä¢ Remember to unpersist() when done to free memory\n",
        "'''\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "sc = spark.sparkContext\n",
        "price_tiers = {\n",
        "    \"economy\": 1.0, \"standard\": 1.5,\n",
        "    \"premium\": 2.0, \"luxury\": 3.0\n",
        "}\n",
        "bc_tiers = sc.broadcast(price_tiers)\n",
        "\n",
        "def calculate_price_udf(tier, base_price):\n",
        "    multiplier = bc_tiers.value.get(tier, 1.0)\n",
        "    return base_price * multiplier\n",
        "\n",
        "calc_price = udf(calculate_price_udf, StringType())\n",
        "\n",
        "products = spark.createDataFrame([\n",
        "      (\"Product A\", \"economy\", 100),\n",
        "      (\"Product B\", \"premium\", 150),\n",
        "      (\"Product C\", \"luxury\", 200),\n",
        "      (\"Product D\", \"standard\", 120)\n",
        "  ], [\"name\", \"tier\", \"base_price\"])\n",
        "\n",
        "products_priced = products.withColumn(\n",
        "    \"final_price\",\n",
        "    calc_price(col(\"tier\"), col(\"base_price\"))\n",
        ")\n",
        "products_priced.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dI6R6JwRiaT6"
      },
      "source": [
        "---\n",
        "**HINTS** (Check execution plan with df.explain() to verify hints)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kL-WQazbiaGs"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PARTITIONING STRATEGY HINTS\n",
        "# ============================================================================\n",
        "\n",
        "# COALESCE - Reduces partitions to a specified number.\n",
        "result = spark.sql(\"\"\"\n",
        "\tSELECT /*+ COALESCE(10) */\n",
        "\tCASE WHEN score > 80 THEN 'A' ELSE 'B' END AS grade,\n",
        "\tstudent_id\n",
        "\tFROM large_table\n",
        "\"\"\")\n",
        "# In Spark SQL, query hints must appear immediately after the SELECT keyword.\n",
        "\n",
        "# Hints are applied before transformations like select, just like SQL hints apply at query planning time.\n",
        "df_result = df.hint(\"coalesce\", 10)\n",
        "# Or use direct method:\n",
        "df_result = df.coalesce(10)\n",
        "\n",
        "# REPARTITION - Repartition your dataframe to the specified number of partitions.\n",
        "result = spark.sql(\"\"\"\n",
        "  SELECT /*+ COALESCE(10), REPARTITION(20) */\n",
        "  CASE WHEN score > 80 THEN 'A' ELSE 'B' END AS grade,\n",
        "  student_id\n",
        "  FROM large_table\n",
        "\"\"\")\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "result_df = (\n",
        "    df\n",
        "    .hint(\"COALESCE\", 10)\n",
        "    .hint(\"REPARTITION\", 20)\n",
        "    .select(\n",
        "        F.when(F.col(\"score\") > 80, F.lit(\"A\"))\n",
        "         .otherwise(F.lit(\"B\"))\n",
        "         .alias(\"grade\"),\n",
        "        F.col(\"student_id\")\n",
        "    )\n",
        ")\n",
        "\n",
        "# REPARTITION_BY_RANGE - Similar to REPARTITION but uses data ranges for partitioning.\n",
        "result = spark.sql(\"\"\"\n",
        "  SELECT /*+ REPARTITION(50, dept, country) */ *\n",
        "  FROM employees\n",
        "\"\"\")\n",
        "\n",
        "df_result = df.hint(\"repartition\", 50, \"dept\", \"country\")\n",
        "# Or:\n",
        "df_result = df.repartition(50, \"dept\", \"country\")\n",
        "\n",
        "# REBALANCE - Used to rebalance  the query result output partitions, so that every partition is of a reasonable size.\n",
        "spark.sql(\"SELECT /*+ REBALANCE */ * FROM t\")\n",
        "spark.sql(\"SELECT /*+ REBALANCE(3) */ * FROM t\")\n",
        "spark.sql(\"SELECT /*+ REBALANCE(c) */ * FROM t\")\n",
        "spark.sql(\"SELECT /*+ REBALANCE(3, c) */ * FROM t\")\n",
        "\n",
        "df_rebalanced = df.hint(\"REBALANCE\") # Rebalance using default Spark behavior -> spark.sql.shuffle.partitions\n",
        "df_rebalanced = df.hint(\"REBALANCE\", 3) # Rebalance to 3 partitions\n",
        "df_rebalanced = df.hint(\"REBALANCE\", \"c\") # Rebalance using column c\n",
        "df_rebalanced = df.hint(\"REBALANCE\", 3, \"c\") # Rebalance to 3 partitions using column c\n",
        "\n",
        "# ============================================================================\n",
        "# JOIN STRATEGY HINTS\n",
        "# ============================================================================\n",
        "# When different join strategy hints are specified on both sides of a join, Spark prioritizes hints in the following order: BROADCAST over MERGE over SHUFFLE_HASH over SHUFFLE_REPLICATE_NL.\n",
        "# When both sides are specified with the BROADCAST hint or the SHUFFLE_HASH hint, Spark will pick the build side based on the join type and the sizes of the relations.\n",
        "\n",
        "\n",
        "# BROADCAST / BROADCASTJOIN / MAPJOIN - Forces broadcast join (hash join)\n",
        "result = spark.sql(\"\"\"\n",
        "  SELECT /*+ BROADCAST(small_table) */ *\n",
        "  FROM large_table\n",
        "  JOIN small_table ON large_table.id = small_table.id\n",
        "\"\"\")\n",
        "df = large_df.join(small_df.hint(\"broadcast\"), large_df.id == small_df.id, \"inner\")\n",
        "# If both sides of the join have the broadcast hints, the one with the smaller size (based on stats) will be broadcast.\n",
        "\n",
        "# MERGE/ SHUFFLE_MERGE/ MERGEJOIN - default join for large datasets, Both DataFrames are shuffled on the join key.\n",
        "spark.sql(\"SELECT /*+ SHUFFLE_MERGE(t1) */ * FROM t1 INNER JOIN t2 ON t1.key = t2.key\")\n",
        "spark.sql(\"SELECT /*+ MERGEJOIN(t2) */ * FROM t1 INNER JOIN t2 ON t1.key = t2.key\")\n",
        "spark.sql(\"SELECT /*+ MERGE(t1) */ * FROM t1 INNER JOIN t2 ON t1.key = t2.key\")\n",
        "\n",
        "df = t1.hint(\"shuffle_merge\").join(t2, on=\"key\", how=\"inner\")\n",
        "df = t2.hint(\"mergejoin\").join(t1, on=\"key\", how=\"inner\")\n",
        "df = t1.hint(\"merge\").join(t2, on=\"key\", how=\"inner\")\n",
        "\n",
        "# SHUFFLE_HASH - Both DataFrames are shuffled on the join key (like merge join). Instead of sorting, Spark builds a hash map of the smaller side per partition. Probes the hash map for matching rows.\n",
        "result = spark.sql(\"\"\"\n",
        "  SELECT /*+ SHUFFLE_HASH(df1) */ *\n",
        "  FROM df1\n",
        "  JOIN df2 ON df1.key = df2.key\n",
        "\"\"\")\n",
        "df_result = df1.hint(\"shuffle_hash\").join(\n",
        "    df2,\n",
        "    df1.key == df2.key\n",
        ")\n",
        "\n",
        "# SHUFFLE_REPLICATE_NL - The small DataFrame is replicated (copied) to every executor and every partition of the large dataframe. A nested-loop join (cartesian-like) happens per partition.\n",
        "# When no join keys exist Or when join condition is non-equijoin, e.g.: df1.col(\"value\") > df2.col(\"value\")\n",
        "spark.sql(\"\"\"\n",
        "  SELECT /*+ SHUFFLE_REPLICATE_NL(df1) */ *\n",
        "  FROM df1\n",
        "  JOIN df2 ON df1.col > df2.col\n",
        "\"\"\")\n",
        "df_result = df1.hint(\"shuffle_replicate_nl\").join( df2, df1.col > df2.col)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnl6yW5C0BLT"
      },
      "source": [
        "---\n",
        "**Integration with external API**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8BJNkvq0A34"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 16. WORKING WITH EXTERNAL APIS\n",
        "# ============================================================================\n",
        "\n",
        "import requests\n",
        "from datetime import datetime\n",
        "from itertools import product\n",
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "def get_conversion_rates(date, pairs=None):\n",
        "    \"\"\"Fetch conversion rates for currency pairs from API\"\"\"\n",
        "    base_url = \"https://api.bank.com/exchange-rate\"\n",
        "\n",
        "    # If pairs not provided, get all currency combinations\n",
        "    if pairs is None:\n",
        "        currencies_url = \"https://api.bank.com/currencies\"\n",
        "        try:\n",
        "            resp = requests.get(currencies_url, timeout=10)\n",
        "            resp.raise_for_status()\n",
        "            currencies = resp.json().get(\"currencies\", [])\n",
        "            if not currencies:\n",
        "                return []\n",
        "\n",
        "            # Create all combinations (excluding same currency)\n",
        "            pairs = [(src, tgt) for src, tgt in product(currencies, currencies)\n",
        "                     if src != tgt]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to fetch currency list: {e}\")\n",
        "            return []\n",
        "\n",
        "    # Fetch rates for all pairs\n",
        "    rates = []\n",
        "    for src, tgt in pairs:\n",
        "        params = {\"date\": date, \"src\": src, \"target\": tgt}\n",
        "        try:\n",
        "            resp = requests.get(base_url, params=params, timeout=10)\n",
        "            resp.raise_for_status()\n",
        "            data = resp.json()\n",
        "            rate = data.get(\"rate\")\n",
        "            if rate:\n",
        "                rates.append((date, src, tgt, rate))\n",
        "        except Exception as e:\n",
        "            print(f\"Failed for {src}->{tgt}: {e}\")\n",
        "\n",
        "    return rates\n",
        "\n",
        "\n",
        "# Create DataFrame from API data\n",
        "today = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "pairs = [(\"USD\", \"INR\"), (\"EUR\", \"INR\"), (\"GBP\", \"INR\")]\n",
        "rates_data = get_conversion_rates(today, pairs)\n",
        "\n",
        "columns = [\"date\", \"src_currency\", \"target_currency\", \"rate\"]\n",
        "rates_df = spark.createDataFrame(rates_data, columns)\n",
        "\n",
        "# Use broadcast for small lookup tables\n",
        "transactions_df = spark.read.parquet(\"transactions/\")\n",
        "joined_df = transactions_df.join(\n",
        "    broadcast(rates_df),\n",
        "    transactions_df.currency == rates_df.src_currency,\n",
        "    \"left\"\n",
        ")\n",
        "\n",
        "converted_df = joined_df.withColumn(\n",
        "    \"amount_in_inr\",\n",
        "    col(\"amount\") * col(\"rate\")\n",
        ").select(\"txn_id\", \"currency\", \"amount\", \"rate\", \"amount_in_inr\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoIBjWrCP9jv"
      },
      "source": [
        "---\n",
        "**Spark Streaming**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GChyadXP9Vj"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"StreamingExample\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Define Schema\n",
        "schema = StructType([\n",
        "    StructField(\"timestamp\", TimestampType(), True),\n",
        "    StructField(\"user_id\", StringType(), True),\n",
        "    StructField(\"event_type\", StringType(), True),\n",
        "    StructField(\"value\", DoubleType(), True)\n",
        "])\n",
        "\n",
        "# Read Stream from Kafka\n",
        "raw_stream = spark.readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
        "    .option(\"subscribe\", \"events\") \\\n",
        "    .option(\"startingOffsets\", \"latest\") \\\n",
        "    .load()\n",
        "# Use maxFilesPerTrigger and/or maxBytesPerTrigger in the read options to limit the number of files or total data size\n",
        "\n",
        "# Parse and Transform\n",
        "parsed_stream = raw_stream \\\n",
        "    .selectExpr(\"CAST(value AS STRING)\") \\\n",
        "    .select(from_json(col(\"value\"), schema).alias(\"data\")) \\\n",
        "    .select(\"data.*\")\n",
        "\n",
        "# filter() / where() can be used to filter rows based on conditions. Filtering on aggregations is not supported, but row-level filtering like this is supported.\n",
        "\n",
        "# df.filter(F.count(\"x\") > 30)\n",
        "# Aggregations with a condition like count(x) > 30 require grouping and stateful processing. These are supported only with groupBy + agg and proper output modes, not directly inside a simple filter like this.\n",
        "\n",
        "# Actions like show(), collect(), limit() and count() are not supported on streaming DataFrames,\n",
        "\n",
        "# Apply Watermark and Window Aggregation\n",
        "windowed_stream = parsed_stream \\\n",
        "    .withWatermark(\"timestamp\", \"15 minutes\") \\\n",
        "    .groupBy(\n",
        "        window(col(\"timestamp\"), \"5 minutes\", \"1 minute\"),\n",
        "        col(\"event_type\")\n",
        "    ).agg(\n",
        "        count(\"*\").alias(\"event_count\"),\n",
        "        avg(\"value\").alias(\"avg_value\"),\n",
        "        max(\"value\").alias(\"max_value\")\n",
        "    )\n",
        "'''\n",
        "A watermark sets an upper bound on how late a duplicate can arrive, allowing Spark to safely remove old state and control memory usage.\n",
        "‚Ä¢ Watermarks handle late-arriving data in event-time processing\n",
        "‚Ä¢ Define how long to wait for late data before finalizing results\n",
        "‚Ä¢ Based on event time, not processing time\n",
        "‚Ä¢ Allows state cleanup and bounded memory usage\n",
        "‚Ä¢ Trade-off between completeness and latency\n",
        "‚Ä¢ Essential for window operations with late data\n",
        "\n",
        "from pyspark.sql.functions import window, col\n",
        "\n",
        "# Basic Watermark\n",
        "df_with_watermark = df \\\n",
        "    .withWatermark(\"timestamp\", \"10 minutes\")\n",
        "\n",
        "# Watermark with Window Aggregation\n",
        "windowed_counts = df \\\n",
        "    .withWatermark(\"timestamp\", \"10 minutes\") \\\n",
        "    .groupBy(\n",
        "        window(col(\"timestamp\"), \"5 minutes\"),\n",
        "        col(\"user_id\")\n",
        "    ) \\\n",
        "    .count()\n",
        "\n",
        "# Multiple Watermarks in Joins\n",
        "df1_watermarked = df1.withWatermark(\"timestamp\", \"5 minutes\")\n",
        "df2_watermarked = df2.withWatermark(\"timestamp\", \"10 minutes\")\n",
        "\n",
        "joined = df1_watermarked.join(\n",
        "    df2_watermarked,\n",
        "    expr(\"\"\"\n",
        "        user_id = user_id AND\n",
        "        timestamp1 >= timestamp2 AND\n",
        "        timestamp1 <= timestamp2 + interval 1 hour\n",
        "    \"\"\")\n",
        ")\n",
        "\n",
        "# Watermark with Append Mode\n",
        "query = windowed_counts.writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .option(\"checkpointLocation\", \"/checkpoint\") \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "\n",
        "# Understanding Watermark Calculation\n",
        "# Watermark = max_event_time - watermark_delay\n",
        "# Data with event_time < watermark is considered late\n",
        "'''\n",
        "# Write Stream with Checkpointing\n",
        "query = windowed_stream.writeStream \\\n",
        "    .outputMode(\"update\") \\\n",
        "    .format(\"parquet\") \\\n",
        "    .option(\"path\", \"/output/events\") \\\n",
        "    .option(\"checkpointLocation\", \"/checkpoint/events\") \\\n",
        "    .trigger(processingTime='30 seconds') \\\n",
        "    .start()\n",
        "\n",
        "'''\n",
        "Append mode (default) - This is the default mode, where only the new rows added to the Result Table since the last trigger will be outputted to the sink. This is supported for only those queries where rows added to the Result Table are never going to change. Hence, this mode guarantees that each row will be output only once (assuming fault-tolerant sink). For example, queries with only select, where, map, flatMap, filter, join, etc. will support Append mode.\n",
        "Complete mode - The whole Result Table will be outputted to the sink after every trigger. This is supported for aggregation queries.\n",
        "Update mode - (Available since Spark 2.1.1) Only the rows in the Result Table that were updated since the last trigger will be outputted to the sink. More information to be added in future releases.\n",
        "\n",
        "# Append Mode (default) - Only new rows added since last trigger\n",
        "query = df.writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "\n",
        "# Complete Mode - Entire result table output every trigger\n",
        "agg_df = df.groupBy(\"category\").count()\n",
        "\n",
        "query = agg_df.writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "\n",
        "# Update Mode - Only rows updated since last trigger\n",
        "windowed = df \\\n",
        "    .withWatermark(\"timestamp\", \"10 minutes\") \\\n",
        "    .groupBy(\n",
        "        window(col(\"timestamp\"), \"5 minutes\"),\n",
        "        col(\"category\")\n",
        "    ).count()\n",
        "\n",
        "query = windowed.writeStream \\\n",
        "    .outputMode(\"update\") \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "\n",
        "# Mode Compatibility Examples\n",
        "# Append: No aggregation, watermarked aggregation\n",
        "# Complete: All aggregations (memory intensive)\n",
        "# Update: Aggregations with state\n",
        "\n",
        "# Example: Aggregation without Watermark\n",
        "# Must use Complete or Update mode\n",
        "df.groupBy(\"user\").count() \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "\n",
        "---------------------------------------------------------------------------------------------------------\n",
        "\n",
        "Checkpointing provides fault tolerance and exactly-once semantics\n",
        "‚Ä¢ Stores metadata about streaming query progress\n",
        "‚Ä¢ Includes offsets, state information, and query configuration\n",
        "‚Ä¢ Required for stateful operations and production deployments\n",
        "‚Ä¢ Enables restart from last checkpoint after failures\n",
        "‚Ä¢ Checkpoint location must be reliable (HDFS, S3, etc.)\n",
        "\n",
        "# Basic Checkpointing\n",
        "query = df.writeStream \\\n",
        "    .format(\"parquet\") \\\n",
        "    .option(\"path\", \"/output/path\") \\\n",
        "    .option(\"checkpointLocation\", \"/checkpoint/path\") \\\n",
        "    .start()\n",
        "\n",
        "# Multiple Queries with Different Checkpoints\n",
        "query1 = df1.writeStream \\\n",
        "    .option(\"checkpointLocation\", \"/checkpoints/query1\") \\\n",
        "    .format(\"parquet\") \\\n",
        "    .start(\"/output/query1\")\n",
        "\n",
        "query2 = df2.writeStream \\\n",
        "    .option(\"checkpointLocation\", \"/checkpoints/query2\") \\\n",
        "    .format(\"parquet\") \\\n",
        "    .start(\"/output/query2\")\n",
        "\n",
        "# Checkpoint with State Store\n",
        "from pyspark.sql.functions import window, col\n",
        "\n",
        "windowed = df.groupBy(\n",
        "    window(col(\"timestamp\"), \"10 minutes\")\n",
        ").count()\n",
        "\n",
        "query = windowed.writeStream \\\n",
        "    .outputMode(\"update\") \\\n",
        "    .option(\"checkpointLocation\", \"/checkpoint/windowed\") \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "\n",
        "---------------------------------------------------------------------------------------------------------\n",
        "\n",
        "Triggers define when the streaming query should process data\n",
        "‚Ä¢ Default: micro-batch processing as data arrives\n",
        "‚Ä¢ Types: ProcessingTime, Once, Continuous, AvailableNow\n",
        "‚Ä¢ Affects latency vs throughput tradeoff\n",
        "‚Ä¢ Continuous mode offers low latency (experimental)\n",
        "\n",
        "# Default Trigger (as fast as possible)\n",
        "query = df.writeStream \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "\n",
        "# Fixed Interval Trigger\n",
        "query = df.writeStream \\\n",
        "    .trigger(processingTime='10 seconds') \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "\n",
        "# One-time Trigger (batch-like)\n",
        "query = df.writeStream \\\n",
        "    .trigger(once=True) \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "\n",
        "# Available Now (process all available data)\n",
        "query = df.writeStream \\\n",
        "    .trigger(availableNow=True) \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "\n",
        "# Continuous Trigger (low latency)\n",
        "query = df.writeStream \\\n",
        "    .trigger(continuous='1 second') \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "'''\n",
        "\n",
        "# Monitor Query\n",
        "query.awaitTermination()\n",
        "\n",
        "# Additional Monitoring\n",
        "print(f\"Query ID: {query.id}\")\n",
        "print(f\"Status: {query.status}\")\n",
        "print(f\"Recent Progress: {query.recentProgress}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
